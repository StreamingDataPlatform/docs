(window.webpackJsonp=window.webpackJsonp||[]).push([[12],{155:function(e,t,a){"use strict";a.d(t,"a",(function(){return p})),a.d(t,"b",(function(){return h}));var n=a(0),r=a.n(n);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function s(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function c(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=r.a.createContext({}),m=function(e){var t=r.a.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):s(s({},t),e)),a},p=function(e){var t=m(e.components);return r.a.createElement(l.Provider,{value:t},e.children)},b={inlineCode:"code",wrapper:function(e){var t=e.children;return r.a.createElement(r.a.Fragment,{},t)}},d=r.a.forwardRef((function(e,t){var a=e.components,n=e.mdxType,o=e.originalType,i=e.parentName,l=c(e,["components","mdxType","originalType","parentName"]),p=m(a),d=n,h=p["".concat(i,".").concat(d)]||p[d]||b[d]||o;return a?r.a.createElement(h,s(s({ref:t},l),{},{components:a})):r.a.createElement(h,s({ref:t},l))}));function h(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var o=a.length,i=new Array(o);i[0]=d;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s.mdxType="string"==typeof e?e:n,i[1]=s;for(var l=2;l<o;l++)i[l]=a[l];return r.a.createElement.apply(null,i)}return r.a.createElement.apply(null,a)}d.displayName="MDXCreateElement"},208:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/controller-diagram-2bacf19ec89bbc9ec5775070b1ffc6a2.png"},209:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/request-process-flow-77572d6c1c586af98fef956e6f1631a0.png"},210:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/transaction-management-678a9af90d4052a571f375758cbca6d4.png"},79:function(e,t,a){"use strict";a.r(t),a.d(t,"frontMatter",(function(){return i})),a.d(t,"metadata",(function(){return s})),a.d(t,"toc",(function(){return c})),a.d(t,"default",(function(){return m}));var n=a(3),r=a(7),o=(a(0),a(155)),i={title:"Pravega Controller Service"},s={unversionedId:"pravega/controller-service",id:"pravega/controller-service",isDocsHomePage:!1,title:"Pravega Controller Service",description:"\x3c!--",source:"@site/docs/pravega/controller-service.md",slug:"/pravega/controller-service",permalink:"/docs/docs/pravega/controller-service",editUrl:"https://github.com/claudiofahey/pravega/edit/docusaurus/documentation/src/docs/controller-service.md",version:"current",sidebar:"mainSidebar",previous:{title:"Segment Containers in a Pravega Cluster",permalink:"/docs/docs/pravega/segment-containers"},next:{title:"Pravega Streaming Service Wire Protocol",permalink:"/docs/docs/pravega/wire-protocol"}},c=[{value:"Stream Management",id:"stream-management",children:[]},{value:"Cluster Management",id:"cluster-management",children:[]},{value:"Service Endpoints",id:"service-endpoints",children:[{value:"gRPC",id:"grpc",children:[]},{value:"REST",id:"rest",children:[]}]},{value:'Pravega Controller Service <a name ="controller-service"></a>',id:"pravega-controller-service",children:[]},{value:"Stream Metadata Store",id:"stream-metadata-store",children:[{value:"Stream Metadata",id:"stream-metadata",children:[]},{value:"Stream Store Caching",id:"stream-store-caching",children:[]}]},{value:"Stream Buckets",id:"stream-buckets",children:[{value:"Retention Set",id:"retention-set",children:[]}]},{value:"Controller Cluster Listener",id:"controller-cluster-listener",children:[]},{value:"Host Store",id:"host-store",children:[]},{value:"Background Workers",id:"background-workers",children:[{value:"Task Framework",id:"task-framework",children:[]},{value:"Event Processor Framework",id:"event-processor-framework",children:[]}]},{value:"Stream Operations",id:"stream-operations",children:[{value:"Create Stream",id:"create-stream",children:[]},{value:"Update Stream",id:"update-stream",children:[]},{value:"Scale Stream",id:"scale-stream",children:[]},{value:"Truncate Stream",id:"truncate-stream",children:[]},{value:"Seal Stream",id:"seal-stream",children:[]},{value:"Delete Stream",id:"delete-stream",children:[]}]},{value:"Stream Policy Manager",id:"stream-policy-manager",children:[{value:"Scaling Infrastructure",id:"scaling-infrastructure",children:[]},{value:"Retention Infrastructure",id:"retention-infrastructure",children:[]}]},{value:"Transaction Manager",id:"transaction-manager",children:[{value:"Create Transaction",id:"create-transaction",children:[]},{value:"Commit Transaction",id:"commit-transaction",children:[]},{value:"Abort Transaction",id:"abort-transaction",children:[]},{value:"Ping Transaction",id:"ping-transaction",children:[]},{value:"Transaction Timeout Management",id:"transaction-timeout-management",children:[]}]},{value:"Segment Container to Host Mapping",id:"segment-container-to-host-mapping",children:[]}],l={toc:c};function m(e){var t=e.components,i=Object(r.a)(e,["components"]);return Object(o.b)("wrapper",Object(n.a)({},l,i,{components:t,mdxType:"MDXLayout"}),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#introduction"},"Introduction")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#architecture"},"Architecture"),Object(o.b)("ul",{parentName:"li"},Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#stream-management"},"Stream Management")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#cluster-management"},"Cluster Management")))),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#system-diagram"},"System Diagram")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#components"},"Components"),Object(o.b)("ul",{parentName:"li"},Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#service-endpoints"},"Service Endpoints")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#controller-service"},"Controller Service")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#stream-metadata-store"},"Stream Metadata Store"),Object(o.b)("ul",{parentName:"li"},Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#stream-metadata"},"Stream Metadata")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#stream-store-caching"},"Stream Store Caching")))),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#stream-buckets"},"Stream Buckets")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#controller-cluster-listener"},"Controller Cluster Listener")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#host-store"},"Host Store")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#background-workers"},"Background workers")))),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#roles-and-responsibilities"},"Roles and Responsibilities"),Object(o.b)("ul",{parentName:"li"},Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#stream-operations"},"Stream Operations"),Object(o.b)("ul",{parentName:"li"},Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#stream-state"},"Stream State")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#create-stream"},"Create Stream")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#update-stream"},"Update Stream")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#scale-stream"},"Scale Stream")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#truncate-stream"},"Truncate Stream")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#seal-stream"},"Seal Stream")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#delete-stream"},"Delete Stream")))),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#stream-policy-manager"},"Stream Policy Manager"),Object(o.b)("ul",{parentName:"li"},Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#scaling-infrastructure"},"Scaling Infrastructure")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#retention-infrastructure"},"Retention Infrastructure")))),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#transaction-manager"},"Transaction Manager"),Object(o.b)("ul",{parentName:"li"},Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#create-transaction"},"Create Transaction")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#commit-transaction"},"Commit Transaction")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#abort-transaction"},"Abort Transaction")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#ping-transaction"},"Ping Transaction")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#transaction-timeout-management"},"Transaction Timeout Management")))),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#segment-container-to-host-mapping"},"Segment Container to Host Mapping")))),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"#resources"},"Resources"))),Object(o.b)("h1",{id:"introduction"},"Introduction"),Object(o.b)("p",null,"The Controller Service is a core component of Pravega that implements\nthe control plane. It acts as the central coordinator and manager for\nvarious operations performed in the cluster, the major two\ncategories are: ",Object(o.b)("strong",{parentName:"p"},"Stream Management")," and ",Object(o.b)("strong",{parentName:"p"},"Cluster Management"),"."),Object(o.b)("p",null,"The Controller Service, referred to simply as ",Object(o.b)("strong",{parentName:"p"},"Controller")," henceforth, is\nresponsible for providing the abstraction of a Pravega ",Object(o.b)("a",{parentName:"p",href:"/docs/docs/pravega/pravega-concepts#streams"},"Stream"),", which is the main abstraction that Pravega exposes to applications. A Pravega Stream\ncomprises one or more Stream ",Object(o.b)("a",{parentName:"p",href:"/docs/docs/pravega/pravega-concepts#stream-segments"},"Segments"),". Each Stream Segment is an append-only data\nstructure that stores a sequence of bytes. A Stream Segment on its own is\nagnostic to the presence of other Stream Segments and is not aware of its logical\nrelationship with its peer Stream Segments. The Segment Store, which owns and\nmanages these Stream Segments, does not have any notion of a Stream.  A Stream is a logical view built by the Controller and consists of a dynamically changing set of Stream Segments that satisfy a predefined set of invariants. The Controller provides the Stream abstraction and\norchestrates all lifecycle operations on a Pravega Stream while ensuring its consistency."),Object(o.b)("p",null,"The Controller plays a central role in the lifecycle of a Stream:\n",Object(o.b)("em",{parentName:"p"},"creation"),", ",Object(o.b)("em",{parentName:"p"},"updation"),", ",Object(o.b)("em",{parentName:"p"},"truncation"),", ",Object(o.b)("em",{parentName:"p"},"sealing"),", ",Object(o.b)("a",{parentName:"p",href:"/docs/docs/pravega/pravega-concepts#elastic-streams-auto-scaling"},Object(o.b)("em",{parentName:"a"},"scaling"))," and ",Object(o.b)("em",{parentName:"p"},"deletion"),".  To implement these operations, the Controller manages both a Stream's metadata and its associated Stream Segments. For example, as part of Stream\u2019s\nlifecycle, new segments can be created and existing segments can be sealed. The\nController decides on performing these operations by ensuring the availability and consistency of the Streams for the clients accessing them."),Object(o.b)("h1",{id:"architecture"},"Architecture"),Object(o.b)("p",null,"The Controller Service is made up of one or more instances of\nstateless worker nodes. Each new Controller instance can be invoked independently and becomes part of Pravega cluster by merely pointing to the same ",Object(o.b)("a",{parentName:"p",href:"https://zookeeper.apache.org/"},"Apache Zookeeper"),". For high availability, it is advised to have\nmore than one instance of Controller service per cluster."),Object(o.b)("p",null,"Each Controller instance is capable of working independently and uses a\nshared persistent store as the source of truth for all state-owned and\nmanaged by Controller service. We currently use Apache ZooKeeper as the\nstore for persisting all metadata consistently.  Each instance comprises\nvarious subsystems which are responsible for performing specific\noperations on different categories of metadata. These subsystems include\ndifferent ",Object(o.b)("em",{parentName:"p"},"API endpoints, metadata store handles, policy managers")," and\n",Object(o.b)("em",{parentName:"p"},"background workers"),"."),Object(o.b)("p",null,"The Controller exposes two endpoints which can be used to interact with\nThe Pravega Controller Service. The first port is for providing programmatic\naccess for Pravega clients and is implemented as an ",Object(o.b)("inlineCode",{parentName:"p"},"RPC")," using ",Object(o.b)("a",{parentName:"p",href:"https://grpc.io/"},"gRPC"),". The\nother endpoint is for administrative operations and is implemented as a\n",Object(o.b)("inlineCode",{parentName:"p"},"REST")," endpoint."),Object(o.b)("h2",{id:"stream-management"},"Stream Management"),Object(o.b)("p",null,'The Controller owns and manages the concept of Stream and is\nresponsible for maintaining "metadata" and "lifecycle" operations (',Object(o.b)("em",{parentName:"p"},"creating, updating, scaling,\ntruncating, sealing")," and ",Object(o.b)("em",{parentName:"p"},"deleting Streams"),") for each Pravega Stream."),Object(o.b)("p",null,"The Stream management can be divided into the following three categories:"),Object(o.b)("ol",null,Object(o.b)("li",{parentName:"ol"},Object(o.b)("strong",{parentName:"li"},"Stream Abstraction"),": A Stream can be viewed as a series of dynamically changing segment sets\nwhere the Stream transitions from one set of consistent segments to the\nnext. The Controller is the place for creating and managing Stream abstraction.\nThe Controller decides when and how a Stream transitions from one state to another and is responsible\nfor performing these transitions by keeping the state of the Stream consistent and available.\nThese transitions are governed user-defined policies that the\nController enforces. Consequently, as part of Stream management, the\nController also performs roles of Policy Manager for policies like\nretention and scaling.")),Object(o.b)("ol",{start:2},Object(o.b)("li",{parentName:"ol"},Object(o.b)("p",{parentName:"li"},Object(o.b)("strong",{parentName:"p"},"Policy Management"),": The Controller is responsible for storing and enforcing user-defined Stream policies by actively monitoring the state of the Stream. In Pravega we\nhave two policies that users can define, namely ",Object(o.b)("a",{parentName:"p",href:"https://github.com/pravega/pravega/blob/master/client/src/main/java/io/pravega/client/stream/ScalingPolicy.java"},Object(o.b)("strong",{parentName:"a"},"Scaling")," ",Object(o.b)("strong",{parentName:"a"},"Policy"))," and\n",Object(o.b)("a",{parentName:"p",href:"https://github.com/pravega/pravega/blob/master/client/src/main/java/io/pravega/client/stream/RetentionPolicy.java"},Object(o.b)("strong",{parentName:"a"},"Retention")," ",Object(o.b)("strong",{parentName:"a"},"Policy")),"."),Object(o.b)("pre",{parentName:"li"},Object(o.b)("code",{parentName:"pre"},"   - **Scaling policy** describes if and under what circumstances a Stream should automatically scale its number of segments.  \n   - **Retention policy** describes a policy about how much data to retain within a Stream based on **time** (*Time-based Retention*) and data **size** (*Size-based Retention*).\n"))),Object(o.b)("li",{parentName:"ol"},Object(o.b)("p",{parentName:"li"},Object(o.b)("a",{parentName:"p",href:"/docs/docs/pravega/pravega-concepts#transactions"},Object(o.b)("strong",{parentName:"a"},"Transaction"))," ",Object(o.b)("strong",{parentName:"p"},"Management"),": Implementing Transactions requires the manipulation of Stream Segments. With\neach Transaction, Pravega creates a set of Transaction segments, which\nare later merged onto the Stream Segments upon commit or discarded upon\naborts. The Controller performs the role of Transaction manager and is\nresponsible for creating and committing Transactions on a given Stream.\nUpon creating Transactions, Controller also tracks Transaction timeouts\nand aborts transactions whose timeouts have elapsed. Details of\nTransaction management can be found later in the ",Object(o.b)("a",{parentName:"p",href:"#transaction-manager"},"Transactions")," section."))),Object(o.b)("h2",{id:"cluster-management"},"Cluster Management"),Object(o.b)("p",null,"The Controller is responsible for managing the Segment Store cluster. Controller manages the\nlifecycle of Segment Store nodes as they are added to/removed from the\ncluster and performs redistribution of segment containers across\navailable Segment Store nodes."),Object(o.b)("h1",{id:"system-diagram"},"System Diagram"),Object(o.b)("p",null,"The following diagram shows the main components of a Controller process.\nThe elements of the diagram are discussed in detail in the following sections."),Object(o.b)("p",null," ",Object(o.b)("img",{alt:"Controller system_diagram",src:a(208).default})),Object(o.b)("p",null,Object(o.b)("em",{parentName:"p"},"Controller Process Diagram")),Object(o.b)("h1",{id:"components"},"Components"),Object(o.b)("h2",{id:"service-endpoints"},"Service Endpoints"),Object(o.b)("p",null,"There are two ports exposed by Controller: ",Object(o.b)("strong",{parentName:"p"},"Client-Controller API")," and\n",Object(o.b)("strong",{parentName:"p"},"Administration API"),". The client Controller communication is implemented as ",Object(o.b)("inlineCode",{parentName:"p"},"RPC")," which\nexposes API to perform all Stream related control plane operations.\nApart from this Controller also exposes an administrative API implemented as ",Object(o.b)("inlineCode",{parentName:"p"},"REST"),"."),Object(o.b)("p",null,"Each endpoint performs the appropriate call to the Pravega Controller Service ",Object(o.b)("em",{parentName:"p"},"backend subsystem"),"\nwhich has the actual implementation for various operations like create, read, update and\ndelete (CRUD) on entities owned and managed by Controller."),Object(o.b)("h3",{id:"grpc"},"gRPC"),Object(o.b)("p",null,"Client Controller communication endpoint is implemented as a ",Object(o.b)("a",{parentName:"p",href:"https://grpc.io/"},Object(o.b)("inlineCode",{parentName:"a"},"gRPC")),"\ninterface. Please check the complete list of ",Object(o.b)("a",{parentName:"p",href:"https://github.com/pravega/pravega/blob/master/shared/controller-api/src/main/proto/Controller.proto"},"API"),". This exposes API used by Pravega clients (Readers, Writers and Stream\nManager) and enables Stream management. Requests enabled by this API\ninclude ",Object(o.b)("em",{parentName:"p"},"creating, modifying,")," and ",Object(o.b)("em",{parentName:"p"},"deleting")," Streams.\nThe underlying ",Object(o.b)("inlineCode",{parentName:"p"},"gRPC")," framework provides both ",Object(o.b)("strong",{parentName:"p"},Object(o.b)("em",{parentName:"strong"},"synchronous"))," and ",Object(o.b)("strong",{parentName:"p"},Object(o.b)("em",{parentName:"strong"},"asynchronous"))," programming models.\nWe use the asynchronous model in our client Controller interactions so that the client thread does not block on the response from the server.  "),Object(o.b)("p",null,"To be able to append to and read data from Streams, Writers and Readers\nquery Controller to get active Stream Segment sets, successor and predecessor\nStream Segments while working with a Stream. For Transactions, the client uses\nspecific API calls to request Controller to ",Object(o.b)("a",{parentName:"p",href:"#create-transaction"},Object(o.b)("em",{parentName:"a"},"create")),", ",Object(o.b)("a",{parentName:"p",href:"#commit-transaction"},Object(o.b)("em",{parentName:"a"},"commit")),", ",Object(o.b)("a",{parentName:"p",href:"#abort-transaction"},Object(o.b)("em",{parentName:"a"},"abort"))," and ",Object(o.b)("a",{parentName:"p",href:"#ping-transaction"},Object(o.b)("em",{parentName:"a"},"ping")),"\nTransactions."),Object(o.b)("h3",{id:"rest"},"REST"),Object(o.b)("p",null,"For administration, the Controller implements and exposes a ",Object(o.b)("inlineCode",{parentName:"p"},"REST"),"\ninterface. This includes API calls for Stream management as well as\nother administration API primarily dealing with ",Object(o.b)("em",{parentName:"p"},"creation")," and ",Object(o.b)("em",{parentName:"p"},"deletion")," of\n",Object(o.b)("a",{parentName:"p",href:"/docs/docs/pravega/pravega-concepts#streams"},Object(o.b)("strong",{parentName:"a"},"Scopes")),". We use ",Object(o.b)("a",{parentName:"p",href:"https://swagger.io"},"swagger")," to describe our ",Object(o.b)("inlineCode",{parentName:"p"},"REST")," API. Please see, the swagger ",Object(o.b)("a",{parentName:"p",href:"https://github.com/pravega/pravega/tree/master/shared/controller-api/src/main/swagger"},Object(o.b)("inlineCode",{parentName:"a"},"yaml"))," file."),Object(o.b)("h2",{id:"pravega-controller-service"},"Pravega Controller Service ",Object(o.b)("a",{name:"controller-service"})),Object(o.b)("p",null,"This is the backend layer behind the Controller endpoints ",Object(o.b)("inlineCode",{parentName:"p"},"gRPC")," and\n",Object(o.b)("inlineCode",{parentName:"p"},"REST"),". All the business logic required to serve Controller API calls are\nimplemented here. This layer contains handles to all other subsystems like the various store implementations\n(",Object(o.b)("a",{parentName:"p",href:"#stream-metadata-store"},"Stream store"),", ",Object(o.b)("a",{parentName:"p",href:"#host-store"},"Host store")," and Checkpoint store) and background processing frameworks (",Object(o.b)("a",{parentName:"p",href:"#task-framework"},"Task")," and ",Object(o.b)("a",{parentName:"p",href:"#event-processor-framework"},"Event Processor framework"),").\nStores are interfaces that provide access to various types of metadata managed by Controller. Background\nprocessing frameworks are used to perform asynchronous processing that typically implements workflows involving metadata updates\nand requests to Segment Store."),Object(o.b)("h2",{id:"stream-metadata-store"},"Stream Metadata Store"),Object(o.b)("p",null,"A Stream is a dynamically changing sequence of Stream Segments, where regions of\nthe Routing Key space map to open Stream Segments. As the set of Segments of a\nStream changes, so do the mapping of the Routing Key space to Segments."),Object(o.b)("p",null,"A set of Segments is consistent iff the union of key space ranges mapping\nto Segments in the set covers the entire key space and the key space\nranges are disjoint."),Object(o.b)("p",null,"For example, suppose a set ",Object(o.b)("strong",{parentName:"p"},"S")," = {",Object(o.b)("strong",{parentName:"p"},"S"),Object(o.b)("sub",null,Object(o.b)("strong",{parentName:"p"},"1")),", ",Object(o.b)("strong",{parentName:"p"},"S"),Object(o.b)("sub",null,"2"),", ",Object(o.b)("strong",{parentName:"p"},"S"),Object(o.b)("sub",null,Object(o.b)("strong",{parentName:"p"},"3")),"}, such that:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Region ","[","0, 0.3) maps to Segment ",Object(o.b)("strong",{parentName:"li"},"S"),Object(o.b)("sub",null,Object(o.b)("strong",{parentName:"li"},"1")),"."),Object(o.b)("li",{parentName:"ul"},"Region ","[","0.3, 0.6) maps to Segment ",Object(o.b)("strong",{parentName:"li"},"S"),Object(o.b)("sub",null,Object(o.b)("strong",{parentName:"li"},"2")),"."),Object(o.b)("li",{parentName:"ul"},"Region ","[","0.6, 1.0) maps to Segment ",Object(o.b)("strong",{parentName:"li"},"S"),Object(o.b)("sub",null,Object(o.b)("strong",{parentName:"li"},"3")),".")),Object(o.b)("p",null,Object(o.b)("strong",{parentName:"p"},"S")," is a consistent Segment set.  "),Object(o.b)("p",null,"A Stream goes through transformations as it scales over time. A Stream\nstarts with an initial set of Segments that is determined by the Stream\nconfiguration when created and it transitions to new sets of Segments as\nscale operations are performed on the Stream. Each generation of\nSegments that constitute the Stream at any given point in time are\nconsidered to belong to an ",Object(o.b)("strong",{parentName:"p"},"epoch"),". A Stream starts with initial epoch 0\nand upon each transition, it moves ahead in its epochs\nto describe the change in generation of Segments in the Stream."),Object(o.b)("p",null,"The Controller maintains the Stream: it stores the information about all epochs that constitute a given Stream and also about their transitions. The metadata store is designed to persist the information pertaining to Stream Segments, and to enable queries over this information."),Object(o.b)("p",null,"Apart from the epoch information, it keeps some additional metadata,\nsuch as ",Object(o.b)("a",{parentName:"p",href:"#stream-state"},"state")," and its ",Object(o.b)("a",{parentName:"p",href:"#stream-policy-manager"},"policies")," and ongoing Transactions on the Stream. Various sub-components of Controller access the stored metadata for each\nStream via a well-defined\n",Object(o.b)("a",{parentName:"p",href:"https://github.com/pravega/pravega/blob/master/controller/src/main/java/io/pravega/controller/store/stream/StreamMetadataStore.java"},"interface"),".\nWe currently have two concrete implementations of the Stream store\ninterface: ",Object(o.b)("em",{parentName:"p"},"in-memory")," and ",Object(o.b)("em",{parentName:"p"},"Zookeeper")," backed stores. Both share a common\nbase implementation that relies on Stream objects for providing\nstore-type specific implementations for all Stream-specific metadata.\nThe base implementation of Stream store creates and caches these Stream\nobjects."),Object(o.b)("p",null,"The Stream objects implement a store/Stream interface. The concrete\nStream implementation is specific to the store type and is responsible\nfor implementation of store specific methods. We have a common base implementation of all store types\nthat provide optimistic concurrency. This base class encapsulates the\nlogic for queries against Stream store for all concrete stores that\nsupport Compare And Swap (CAS). We currently use Zookeeper as our\nunderlying store which also supports CAS. We store all Stream metadata\nin a hierarchical fashion under Stream specific ",Object(o.b)("strong",{parentName:"p"},"znodes")," (ZooKeeper\ndata nodes)."),Object(o.b)("p",null,"For the ZooKeeper-based store, we structure our metadata into different\ngroups to support a variety of queries against this metadata. All Stream\nspecific metadata is stored under a scoped/Stream root node. Queries\nagainst this metadata include, but not limited to, querying segment sets\nthat form the Stream at different points in time, segment specific\ninformation, segment predecessors and successors. Refer to ",Object(o.b)("a",{parentName:"p",href:"https://github.com/pravega/pravega/blob/master/controller/src/main/java/io/pravega/controller/store/stream/StreamMetadataStore.java"},"Stream metadata")," interface for details about API exposed by Stream metadata\nstore."),Object(o.b)("h3",{id:"stream-metadata"},"Stream Metadata"),Object(o.b)("p",null,"Clients need information about what Segments constitute a Stream to start their processing and they obtain it from the epoch information the Controller stores in the stream store. Clients need the ability to query and find Stream Segments at any of the three cases efficiently:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"A Reader client typically starts from the ",Object(o.b)("strong",{parentName:"li"},"head")," of the Stream,"),Object(o.b)("li",{parentName:"ul"},"But it might also choose to access the Stream starting from any arbitrarily interesting position."),Object(o.b)("li",{parentName:"ul"},"Writers on the other hand always append to the ",Object(o.b)("strong",{parentName:"li"},"tail")," of the Stream.")),Object(o.b)("p",null,"To enable such queries, the Stream store provides API calls to get the initial set of Stream Segments, Segments at a specific time and current set of Segments."),Object(o.b)("p",null,"As mentioned earlier, a Stream can transition from one set of Stream Segments\n(epoch) to another set of Segments that constitute the Stream. A Stream\nmoves from one epoch to another if there is at least one Stream Segment that is\nsealed and replaced by one or more set of Stream Segments that cover\nprecisely the key space of the sealed Segments. As clients work on\nStreams, they may encounter the end of sealed Stream Segments and consequently\nneed to find new Segments to be able to move forward. To enable the\nclients to query for the next Segments, the stream store exposes via the\nController Service efficient queries for finding immediate successors\nand predecessors for any arbitrary Segment.  "),Object(o.b)("p",null,"To enable serving queries like those mentioned above, we need to efficiently store a time series of these Segment transitions and index them against time. We store this information about the current and historical state of a Stream Segments in a set of records which are designed to optimize on aforementioned queries. Apart from Segment-specific metadata record, the current state of Stream comprises of other metadata types that are described henceforth."),Object(o.b)("h4",{id:"records"},"Records"),Object(o.b)("p",null,"Stream time series is stored as a series of records where each record corresponds to an epoch. As Stream scales and transitions from one epoch to another, a new record is created that has complete information about Stream Segments that forms the epoch."),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},Object(o.b)("strong",{parentName:"p"},"Epoch Records:"),Object(o.b)("br",{parentName:"p"}),"\n",Object(o.b)("em",{parentName:"p"},"Epoch: \u27e8time, list-of-segments-in-epoch\u27e9"),".\nWe store the series of ",Object(o.b)("em",{parentName:"p"},"active")," Stream Segments as they transition from one epoch to another into individual epoch records. Each epoch record corresponds to an epoch which captures a logically consistent (as defined earlier) set of Stream Segments that form the Stream and are valid through the lifespan of the epoch. The epoch record is stored against the epoch number. This record is optimized to answer to query Segments from an epoch with a single call into the store that also enables retrieval of all Stream Segment records in the epoch in ",Object(o.b)("em",{parentName:"p"},"O(1)"),". This record is also used for fetching a Segment-specific record by first computing Stream Segment's creation epoch from Stream Segment ID and then retrieving the epoch record.")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},Object(o.b)("strong",{parentName:"p"},"Current Epoch:"),"\nA special epoch record called ",Object(o.b)("inlineCode",{parentName:"p"},"currentEpoch"),". This is the currently ",Object(o.b)("em",{parentName:"p"},"active")," epoch in the Stream. At any time exactly one epoch is marked as the current epoch. Typically this is the latest epoch with the highest epoch number. However, during an ongoing Stream update workflow like ",Object(o.b)("em",{parentName:"p"},"scale")," or ",Object(o.b)("em",{parentName:"p"},"rolling Transaction"),", the current epoch may not necessarily be the latest epoch. However, at the completion of these workflows, the current epoch is marked as the latest epoch in the stream. The following are three most commonly used scenarios where we want to efficiently know the set of Segments that form the Stream:"),Object(o.b)("ol",{parentName:"li"},Object(o.b)("li",{parentName:"ol"},Object(o.b)("em",{parentName:"li"},"Initial set of Stream Segments"),": The ",Object(o.b)("strong",{parentName:"li"},"head")," of the Stream computation is very efficient as it is typically either the first epoch record or the latest truncation record."),Object(o.b)("li",{parentName:"ol"},Object(o.b)("em",{parentName:"li"},"Current set of Stream Segments"),": The ",Object(o.b)("strong",{parentName:"li"},"tail")," of the Stream is identified by the current epoch record."),Object(o.b)("li",{parentName:"ol"},Object(o.b)("em",{parentName:"li"},"Successors of a particular Stream Segment"),": The successor query results in two calls into the store to retrieve Stream Segment's sealed epoch and the corresponding epoch record. The successors are computed as the Stream Segments that overlap with the given Stream Segment."))),Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},Object(o.b)("strong",{parentName:"p"},"Segment Records:"),"\n",Object(o.b)("em",{parentName:"p"},"Segment-info: \u27e8segmentid, time, keySpace-start, keySpace-end\u27e9"),".\nThe Controller stores Stream Segment information within each epoch record. The Stream Segment ID is composed of two parts and is encoded as a ",Object(o.b)("em",{parentName:"p"},"64 bit")," number. The ",Object(o.b)("em",{parentName:"p"},"high 32 bit")," identifies the creation epoch of the Stream Segment and the ",Object(o.b)("em",{parentName:"p"},"low 32 bit")," uniquely identifies the Stream Segment."))),Object(o.b)("p",null,Object(o.b)("strong",{parentName:"p"},"Note"),": To retrieve Stream Segment record given a Stream Segment ID, we first need to extract the creation epoch and then retrieve the Stream Segment record from the epoch record."),Object(o.b)("h4",{id:"stream-configuration"},"Stream Configuration"),Object(o.b)("p",null," Znode under which Stream configuration is serialized and persisted. A\nStream configuration contains Stream policies that need to be enforced.\n",Object(o.b)("a",{parentName:"p",href:"https://github.com/pravega/pravega/blob/master/client/src/main/java/io/pravega/client/stream/ScalingPolicy.java"},"Scaling policy")," and ",Object(o.b)("a",{parentName:"p",href:"https://github.com/pravega/pravega/blob/master/client/src/main/java/io/pravega/client/stream/RetentionPolicy.java"},"Retention policy")," are supplied by the application at the time of Stream creation and enforced by Controller by monitoring the rate and size of data in the Stream."),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},"The Scaling policy describes if and when to automatically scale is based on incoming traffic conditions into the Stream. The policy supports two\nflavors - ",Object(o.b)("em",{parentName:"p"},"traffic as the rate of Events per second")," and ",Object(o.b)("em",{parentName:"p"},"traffic as the rate of\nbytes per second"),". The application specifies their desired traffic\nrates into each segment by means of scaling policy and the supplied\nvalue is chosen to compute thresholds that determine when to scale a\ngiven Stream.")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},"Retention Policy describes the amount of data that needs to be\nretained into Pravega cluster for this Stream. We support a ",Object(o.b)("em",{parentName:"p"},"time-based"),"\nand a ",Object(o.b)("em",{parentName:"p"},"size-based")," retention policy where applications can choose\nwhether they want to retain data in the Stream by size or by time by\nchoosing the appropriate policy and supplying their desired values."))),Object(o.b)("h4",{id:"stream-state"},"Stream State"),Object(o.b)("p",null," Znode which captures the state of the Stream. It is an enumerator with\nvalues from ",Object(o.b)("em",{parentName:"p"},"creating, active, updating, scaling, truncating, sealing,"),"\nand ",Object(o.b)("em",{parentName:"p"},"sealed"),". Once ",Object(o.b)("em",{parentName:"p"},"active"),", a Stream transition between performing a\nspecific operation and remains ",Object(o.b)("em",{parentName:"p"},"active")," until it is sealed. A transition map is\ndefined in the\n",Object(o.b)("a",{parentName:"p",href:"https://github.com/pravega/pravega/blob/master/controller/src/main/java/io/pravega/controller/store/stream/State.java"},"State"),"\nclass which allows and prohibits various state transitions.\nStream State describes the ",Object(o.b)("em",{parentName:"p"},"current state")," of the Stream. It transitions\nfrom ",Object(o.b)("em",{parentName:"p"},"active")," to respective action based on the action being performed\non the Stream. For example, during scaling the state of the Stream\ntransitions from ",Object(o.b)("em",{parentName:"p"},"active")," to ",Object(o.b)("em",{parentName:"p"},"scaling")," and once scaling completes, it\ntransitions back to ",Object(o.b)("em",{parentName:"p"},"active"),". Stream State is used as a barrier to\nensure only one type of operation is being performed on a given Stream\nat any point in time. Only certain state transitions are allowed and\nare described in the state transition object. Only legitimate state\ntransitions are allowed and any attempt for disallowed transition\nresults in an appropriate exception."),Object(o.b)("h4",{id:"truncation-record"},"Truncation Record"),Object(o.b)("p",null,"The Truncation Record captures the latest truncation point of the Stream which is identified by a ",Object(o.b)("inlineCode",{parentName:"p"},"StreamCut"),". The truncation ",Object(o.b)("inlineCode",{parentName:"p"},"StreamCut")," logically represents the head of the Stream and all the data before this position has been purged completely. For example, let there be ",Object(o.b)("em",{parentName:"p"},Object(o.b)("strong",{parentName:"em"},"n"))," active Segments ",Object(o.b)("strong",{parentName:"p"},"S",Object(o.b)("sub",null,"1")),", ",Object(o.b)("strong",{parentName:"p"},"S",Object(o.b)("sub",null,"2")),", ...,",Object(o.b)("strong",{parentName:"p"},"S",Object(o.b)("sub",null,"n"))," in a Stream. If we truncate this Stream at a ",Object(o.b)("inlineCode",{parentName:"p"},"StreamCut")," ",Object(o.b)("strong",{parentName:"p"},"SC = {S",Object(o.b)("sub",null,"1"),"/O",Object(o.b)("sub",null,"1"),", S",Object(o.b)("sub",null,"2"),"/O",Object(o.b)("sub",null,"2"),",...,S",Object(o.b)("sub",null,"n"),"/O",Object(o.b)("sub",null,"n"),"}"),", then all data before the given ",Object(o.b)("inlineCode",{parentName:"p"},"StreamCut")," could be removed from the durable store. This translates to all the data in Segments that are predecessor Segments of ",Object(o.b)("strong",{parentName:"p"},"S",Object(o.b)("sub",null,"i"))," for ",Object(o.b)("strong",{parentName:"p"},"i ={ 1 to n }"),"; and all the data in Segments ",Object(o.b)("strong",{parentName:"p"},"S",Object(o.b)("sub",null,"i"))," till offset ",Object(o.b)("strong",{parentName:"p"},"O",Object(o.b)("sub",null,"i")),". So we could delete all such predecessor Segments from the Stream and purge all the data before respective offsets from the Segments in ",Object(o.b)("inlineCode",{parentName:"p"},"StreamCut"),"."),Object(o.b)("h4",{id:"sealed-segments-maps"},"Sealed Segments Maps"),Object(o.b)("p",null,"Once the Stream Segments are sealed, the Controller needs to store additional information about the Stream Segment. Presently, we have two types of information:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},"Epoch, the Stream Segment was sealed in.")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},"Size of the Stream Segment at the time of sealing."),Object(o.b)("p",{parentName:"li"},"These records have two different characteristics and are used in different types of queries. For example;"))),Object(o.b)("ol",null,Object(o.b)("li",{parentName:"ol"},Object(o.b)("p",{parentName:"li"},"Sealing epoch is important for querying successor Stream Segments. For each Stream Segment, we store its sealing epoch directly in the metadata store.")),Object(o.b)("li",{parentName:"ol"},Object(o.b)("p",{parentName:"li"},"Stream Segment sizes are used during truncation workflows. For sealed sizes, we store it in a map of Segment to size at the time of sealing.")),Object(o.b)("li",{parentName:"ol"},Object(o.b)("p",{parentName:"li"},"Successor queries are performed on a single Stream Segment whereas truncation workflows work on a group of Stream Segments."),Object(o.b)("p",{parentName:"li"},"This ensures that during truncation we are able to retrieve sealed sizes for multiple Stream Segments with a minimal number of calls into the underlying metadata store. Since we could have an arbitrarily large number of Stream Segments that have been sealed away, we cannot store all of the information in a single map and hence we shard the map and store it. The sharding function we use is to hash the creation epoch and get the shard number."))),Object(o.b)("p",null,"The following are the Transaction Related metadata records:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},Object(o.b)("strong",{parentName:"p"},"Active Transactions"),": Each new Transaction is created under the znode. This stores metadata\ncorresponding to each Transaction as ",Object(o.b)("em",{parentName:"p"},"Active Transaction Record"),". Once a\nTransaction is completed, a new node is created under the global\n",Object(o.b)("em",{parentName:"p"},"Completed Transaction")," znode and removed from under the Stream specific\n",Object(o.b)("em",{parentName:"p"},"Active Transaction")," node.")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},Object(o.b)("strong",{parentName:"p"},"Completed Transactions"),": All completed transactions for all Streams are moved under a separate znode upon completion (via either commit or abort paths). The completion status of Transaction is recorded under this record. To avoid proliferation of stale Transaction records, we provide a cluster level configuration to specify the duration for which a completed Transaction's record should be preserved. Controller periodically garbage collects all Transactions that were completed before the aforesaid configured duration."))),Object(o.b)("h3",{id:"stream-store-caching"},"Stream Store Caching"),Object(o.b)("h4",{id:"in-memory-cache"},"In-memory Cache"),Object(o.b)("p",null,"Since there could be multiple concurrent requests for a given Stream\nbeing processed by the same Controller instance, it is suboptimal to read\nthe value by querying Zookeeper every time. So we have introduced an\n",Object(o.b)("strong",{parentName:"p"},"in-memory cache")," that each Stream store maintains. It caches retrieved\nmetadata per Stream so that there is maximum one copy of the data per\nStream in the cache. There are two in-memory caches:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},Object(o.b)("em",{parentName:"li"},"A cache of\nmultiple Stream objects in the store")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("em",{parentName:"li"},"Cache properties of a Stream in\nthe Stream object"),".")),Object(o.b)("p",null,"The cache can contain both mutable and immutable values. Immutable values, by definition are not a problem. For mutable values, we have introduced a notion of ",Object(o.b)("a",{parentName:"p",href:"#operation-context"},"Operation Context")," and for each new operation, which ensures that during an operation we lazily load latest value of entities into the cache and then use them for all computations within that ",Object(o.b)("a",{parentName:"p",href:"#operation-context"},"Operation's context"),"."),Object(o.b)("h4",{id:"operation-context"},"Operation Context"),Object(o.b)("p",null," At the start of any new operation, we create a context for this operation. The creation of a new operation context invalidates all mutable cached entities for a Stream and each entity is lazily retrieved from the store whenever requested. If a\nvalue is updated during the course of the operation, it is again\ninvalidated in the cache so that other concurrent read/update operations\non the Stream get the new value for their subsequent steps.  "),Object(o.b)("h2",{id:"stream-buckets"},"Stream Buckets"),Object(o.b)("p",null,"To enable some scenarios, we may need the background workers to\nperiodically work on each of the Streams in our cluster to perform\nsome specific action on them. The concept of Stream Bucket is to\ndistribute this periodic background work across all available\nController instances. Controller instances map all available streams in the system into buckets and these buckets are distributed amongst themselves. Hence, all the long-running background work can be uniformly distributed across multiple Controller instances."),Object(o.b)("p",null,Object(o.b)("strong",{parentName:"p"},"Note"),": Number of buckets for a cluster is a fixed (configurable) value for\nthe lifetime of a cluster."),Object(o.b)("p",null,"Each bucket corresponds to a unique znode in\nZookeeper. A qualified scoped Stream name is used to compute a\nhash value to assign the Stream to a bucket. All Controller instances, upon\nstartup, attempt to take ownership of buckets. Upon ",Object(o.b)("em",{parentName:"p"},"failover"),", ownerships\nare transferred, as surviving nodes compete to acquire ownership of\norphaned buckets. The Controller instance which owns a bucket is\nresponsible for all long running scheduled background work corresponding\nto all nodes under the bucket. Presently this entails running periodic\nworkflows to capture ",Object(o.b)("inlineCode",{parentName:"p"},"StreamCut"),"(s) (called Retention-Set) for each Stream at desired frequencies."),Object(o.b)("h3",{id:"retention-set"},"Retention Set"),Object(o.b)("p",null," One retention set per Stream is stored under the corresponding\nbucket/Stream znode. As we compute  ",Object(o.b)("inlineCode",{parentName:"p"},"StreamCut"),"(s) periodically, we keep\npreserving them under this znode. As some automatic truncation is\nperformed, the ",Object(o.b)("inlineCode",{parentName:"p"},"StreamCut"),"(s) that are no longer valid are purged from\nthis set."),Object(o.b)("h2",{id:"controller-cluster-listener"},"Controller Cluster Listener"),Object(o.b)("p",null,"Each node in Pravega Cluster registers itself under a cluster znode as\nan ephemeral node. This includes both Controller and Segment Store\nnodes. Each Controller instance registers a watch on the cluster znode\nto listen for cluster change notifications. These notify about the added and removed nodes."),Object(o.b)("p",null,"One Controller instance assumes leadership amongst all Controller\ninstances. This leader Controller instance is responsible for handling\nSegment Store node change notifications. Based on the changes in topology,\nController instance periodically rebalances segment containers to\nSegment Store node mapping."),Object(o.b)("p",null,"All Controller instances listen for Controller node change\nnotifications. Each Controller instance has multiple sub components that\nimplement the ",Object(o.b)("em",{parentName:"p"},"failover sweeper")," interface. Presently there are three\ncomponents that implement ",Object(o.b)("em",{parentName:"p"},"failover")," sweeper interface namely:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},Object(o.b)("inlineCode",{parentName:"li"},"TaskSweeper")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("inlineCode",{parentName:"li"},"EventProcessors")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("inlineCode",{parentName:"li"},"TransactionSweeper"))),Object(o.b)("p",null,"Whenever a Controller instance is identified to have been removed from the cluster,\nthe cluster listener invokes all registered ",Object(o.b)("em",{parentName:"p"},"failover sweepers")," to\noptimistically try to sweep all the orphaned work previously owned by\nthe failed Controller host."),Object(o.b)("h2",{id:"host-store"},"Host Store"),Object(o.b)("p",null,"The implementation of the Host store interface is used to store ",Object(o.b)("em",{parentName:"p"},"Segment Container")," to ",Object(o.b)("em",{parentName:"p"},"Segment Store"),"\nnode mapping. It exposes API like ",Object(o.b)("inlineCode",{parentName:"p"},"getHostForSegment")," where it computes a\nconsistent hash of Segment ID to compute the owner Segment Container.\nThen based on the container-host mapping, it returns the appropriate URI\nto the caller."),Object(o.b)("h2",{id:"background-workers"},"Background Workers"),Object(o.b)("p",null,"Controller process has two different mechanisms or frameworks for\nprocessing background work. These background works typically entail\nmultiple steps and updates to metadata under a specific metadata root\nentity and potential interactions with one or more Segment Stores."),Object(o.b)("p",null,"We initially started with a simple task framework that gave us the ability to run tasks that take exclusive rights over\na given resource (typically a Stream) and allowed for tasks to ",Object(o.b)("em",{parentName:"p"},"failover")," from one Controller instance to another.\nHowever, this model was limiting in scope and locking semantics, and had no inherent notion of task ordering as\nmultiple tasks could race to acquire working rights (lock) on a resource concurrently. To overcome this limitation we\ncame up with a new infrastructure called ",Object(o.b)("a",{parentName:"p",href:"#event-processor-framework"},Object(o.b)("strong",{parentName:"a"},"Event Processor")),". It is built using Pravega Streams and provides a clear\nmechanism to ensure ",Object(o.b)("strong",{parentName:"p"},Object(o.b)("em",{parentName:"strong"},"mutually exclusive"))," and ",Object(o.b)("strong",{parentName:"p"},Object(o.b)("em",{parentName:"strong"},"ordered processing")),"."),Object(o.b)("h3",{id:"task-framework"},"Task Framework"),Object(o.b)("p",null,"The Task Framework is designed to run exclusive background processing per\nresource such that in case of Controller instance failure, the work can\neasily ",Object(o.b)("em",{parentName:"p"},"failover")," to another Controller instance and brought to\ncompletion. The framework, on its own, does not guarantee idempotent\nprocessing and the author of a task has to handle it if required. The\nmodel of tasks is defined to work on a given resource exclusively, which\nmeans no other task can run concurrently on the same resource. This is\nimplemented by way of a persisted distributed lock implemented on\nZookeeper. The ",Object(o.b)("em",{parentName:"p"},"failover")," of a task is achieved by following a scheme of\nindexing the work a given process is performing. So if a process fails,\nanother process will sweep all outstanding work and attempt to transfer\nownership to itself."),Object(o.b)("p",null,"Note that, upon failure of a Controller process, multiple surviving Controller processes can concurrently attempt sweeping of orphaned tasks. Each of them will index the task in their\nhost-index but exactly one of them will be able to successfully acquire\nthe lock on the resource and hence permission to process the task. The\nparameters for executing a task are serialized and stored under the\nresource."),Object(o.b)("p",null,"Currently, we use the Task Framework only to create Stream tasks. All the\nother background processing is done using the Event Processor Framework."),Object(o.b)("h3",{id:"event-processor-framework"},"Event Processor Framework"),Object(o.b)("p",null,"Event processors Framework is a background worker subsystem which reads\nEvents from an internal Stream and processes it, hence the name Event\nProcessor. In Pravega all Event Processors provides ",Object(o.b)("strong",{parentName:"p"},"at least once\nprocessing")," guarantee. And in its basic flavor, the framework also\nprovides strong ordering guarantees. The Event Processor framework on its own does not guarantee idempotent execution and it is the responsibility of the individual workflows implemented to ensure that the processing is idempotent and safe across multiple executions.\nIn Pravega, there exist different subtypes of Event Processors which allow concurrent processing."),Object(o.b)("p",null,"We create different Event Processors for different kinds of work.\nIn Pravega, there are ",Object(o.b)("em",{parentName:"p"},"three")," different Event Processors:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Committing Transaction,"),Object(o.b)("li",{parentName:"ul"},"Aborting Transactions,"),Object(o.b)("li",{parentName:"ul"},"Processing Stream specific requests (scale, update, seal, etc).")),Object(o.b)("p",null,"Each Controller instance has one Event Processor of each type. The Event Processor Framework\nallows for multiple Readers to be created per Event Processor. All\nReaders for a specific Event Processor across Controller instances share\nthe same Reader Group, which guarantees mutually exclusive distribution\nof work across Controller instances. Each Reader gets a dedicated thread\nwhere it reads the Event, calls for its processing and upon completion\nof processing, updates its ",Object(o.b)("strong",{parentName:"p"},"Checkpoint"),". Events are posted in the Event\nProcessor-specific Stream and are routed to specific Stream Segments using scoped Stream name as the Routing Key."),Object(o.b)("h4",{id:"serial-event-processor"},"Serial Event Processor"),Object(o.b)("p",null,"It essentially reads an Event and initiates its processing and waits on it to complete before moving on to the next Event. This provides strong ordering guarantees in processing. And it\nCheckpoints after processing each Event. ",Object(o.b)("a",{parentName:"p",href:"#commit-transaction"},"Commit Transaction")," is\nimplemented using this Serial Event Processor. The degree of\nparallelism for processing these Events is upper bounded by the number of\nStream Segments in the internal Stream and lower bounded by the number of Readers.\nMultiple Events from across different Streams could land up in the same\nStream Segment due to Serial processing. Serial processing has\na drawback that, processing stalls or flooding of Events from one\nStream could adversely impact latencies for unrelated Streams."),Object(o.b)("h4",{id:"concurrent-event-processor"},"Concurrent Event Processor"),Object(o.b)("p",null,"To overcome the drawbacks of Serial Event Processor, in Pravega we designed ",Object(o.b)("strong",{parentName:"p"},"Concurrent Event Processor"),". Concurrent Event Processor, as the name implies, allows us to process multiple Events concurrently. Here the Reader thread, reads an Event, schedules it\u2019s asynchronous processing and\nreturns to read the next event. There is a ceiling on the number of Events\nthat are concurrently processed at any point in time and as the processing\nof some Event completes, newer Events are allowed to be fetched. The\nCheckpoint scheme here becomes slightly more involved to ensure the guarantee ",Object(o.b)("em",{parentName:"p"},"at least once processing"),"."),Object(o.b)("p",null,"However, with concurrent processing the ordering guarantees get broken. But, it is important to note that only ordering guarantees are needed for processing Events from a Stream and not across Streams. In order to satisfy ordering guarantee, we overlay Concurrent Event processor with ",Object(o.b)("strong",{parentName:"p"},"Serialized Request Handler"),", which queues up Events from the same Stream in the ",Object(o.b)("em",{parentName:"p"},"in-memory queue")," and processes them in order."),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},Object(o.b)("a",{parentName:"p",href:"#commit-transaction"},Object(o.b)("strong",{parentName:"a"},"Commit Transaction"))," processing is implemented on a dedicated Serial Event\nProcessor because strong commit ordering is required by ensuring that\ncommit does not interfere with processing of other kinds of requests on\nthe Stream.")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},Object(o.b)("strong",{parentName:"p"},"Abort Transaction")," processing is implemented on a dedicated Concurrent\nEvent Processor which performs abort processing on Transactions from\nacross Streams concurrently."))),Object(o.b)("p",null,"All other requests for Streams are implemented on a Serialized Request\nHandler which ensures exactly one request per Stream is being processed\nat any given time and there is ordering guarantee within request\nprocessing. However, it allows for concurrent requests from across\nStreams to go on concurrently. Workflows like ",Object(o.b)("em",{parentName:"p"},"scale, truncation, seal,\nupdate")," and ",Object(o.b)("em",{parentName:"p"},"delete Stream")," are implemented for processing on the request\nEvent Processor."),Object(o.b)("h1",{id:"roles-and-responsibilities"},"Roles and Responsibilities"),Object(o.b)("h2",{id:"stream-operations"},"Stream Operations"),Object(o.b)("p",null,"The Controller is the source of truth for all Stream related metadata.\nPravega clients (e.g., ",Object(o.b)("inlineCode",{parentName:"p"},"EventStreamReaders")," and ",Object(o.b)("inlineCode",{parentName:"p"},"EventStreamWriters"),"), in\nconjunction with the Controller, ensure that Stream invariants are\nsatisfied and honored as they work on Streams. The Controller maintains\nthe metadata of Streams, including the entire history of Stream Segments.\nThe Client accessing a Stream need to contact the Controller to obtain\ninformation about Stream Segments."),Object(o.b)("p",null,"Clients query Controller in order to know how to navigate Streams. For\nthis purpose Controller exposes appropriate API to get ",Object(o.b)("em",{parentName:"p"},"active")," Stream Segments,\nsuccessors, predecessors and URIs. These queries\nare served using metadata stored and accessed via Stream store\ninterface."),Object(o.b)("p",null,"The Controller also provides workflows to modify the state and behavior of the\nStream. These workflows include ",Object(o.b)("em",{parentName:"p"},"create, scale, truncation, update, seal,"),"\nand ",Object(o.b)("em",{parentName:"p"},"delete"),". These workflows are invoked both via direct API and in some\ncases as applicable via background policy manager (",Object(o.b)("a",{parentName:"p",href:"https://github.com/pravega/pravega/blob/master/client/src/main/java/io/pravega/client/stream/ScalingPolicy.java"},"Auto Scaling")," and ",Object(o.b)("a",{parentName:"p",href:"https://github.com/pravega/pravega/blob/master/client/src/main/java/io/pravega/client/stream/RetentionPolicy.java"},"Retention"),")."),Object(o.b)("p",null,Object(o.b)("img",{alt:"Request Processing",src:a(209).default})),Object(o.b)("p",null,Object(o.b)("em",{parentName:"p"},"Request Processing Flow Diagram")),Object(o.b)("h3",{id:"create-stream"},"Create Stream"),Object(o.b)("p",null,"The Create Stream is implemented as a task on ",Object(o.b)("strong",{parentName:"p"},"Task Framework"),"."),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},"The Create Stream workflow first sets the initial Stream set to ",Object(o.b)("em",{parentName:"p"},"Creating"),".")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},"Next, it identifies the Segment Containers that should create and own the new Segments for this Stream, and calls ",Object(o.b)("inlineCode",{parentName:"p"},"CreateSegment()")," concurrently for all Segments. Once all ",Object(o.b)("inlineCode",{parentName:"p"},"CreateSegment()"),"(s) return, the ",Object(o.b)("inlineCode",{parentName:"p"},"createStream()")," task completes its execution and change the Stream state to ",Object(o.b)("em",{parentName:"p"},"Active"),". In the case of recoverable failures, the operations are retried.")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},"However, if it is unable to complete any step, the Stream is left dangling in ",Object(o.b)("em",{parentName:"p"},"Creating")," state."))),Object(o.b)("h3",{id:"update-stream"},"Update Stream"),Object(o.b)("p",null,"Update Stream is implemented as a task on ",Object(o.b)("strong",{parentName:"p"},"Serialized Request\nHandler")," over Concurrent Event Processor Framework."),Object(o.b)("ol",null,Object(o.b)("li",{parentName:"ol"},"Update Stream is invoked by an explicit API ",Object(o.b)("inlineCode",{parentName:"li"},"updateStream()")," call into Controller."),Object(o.b)("li",{parentName:"ol"},"It first posts an ",Object(o.b)("em",{parentName:"li"},"Update Request")," Event into request Stream."),Object(o.b)("li",{parentName:"ol"},"Following that it tries to create a temporary update property. If it fails to create the temporary update property, the request is failed and the caller is notified of the failure to update a Stream due to conflict with another ongoing update."),Object(o.b)("li",{parentName:"ol"},"The Event is picked by ",Object(o.b)("strong",{parentName:"li"},"Request Event Processor"),". When the processing\nstarts, the update Stream task expects to find the temporary update\nStream property to be present. If it does not find the property, the\nupdate processing is delayed by pushing Event the back in the in-memory\nqueue until it deems the Event expired. If it finds the property to be\nupdated during this period, before the expiry, the Event is processed\nand ",Object(o.b)("inlineCode",{parentName:"li"},"updateStream()")," operation is performed.")),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Once the update Stream processing starts, it first sets the Stream state to ",Object(o.b)("em",{parentName:"li"},"Updating"),"."),Object(o.b)("li",{parentName:"ul"},"Then, the Stream configuration is updated in the metadata store followed by notifying\nSegment Stores for all ",Object(o.b)("em",{parentName:"li"},"active")," Stream Segments of the Stream, about the change in\npolicy. Now the state is reset to ",Object(o.b)("em",{parentName:"li"},"Active"),".")),Object(o.b)("h3",{id:"scale-stream"},"Scale Stream"),Object(o.b)("p",null,"The Scale can be invoked either by explicit API call (referred to as manual\nscale) or performed automatically based on scale policy (referred to as\n",Object(o.b)("a",{parentName:"p",href:"/docs/docs/pravega/pravega-concepts#elastic-streams-auto-scaling"},"Auto-scaling"),")."),Object(o.b)("p",null,"We first write the Event followed by updating the metadata store to capture our intent to scale a Stream. This step is idempotent and ensures that if an existing ongoing scale operation is in progress, then this attempt to start a new scale is ignored. Also, if there is an ongoing scale operation with a conflicting request input parameter, then the new request is rejected. Which essentially guarantees that there can be exactly one scale operation that can be performed at any given point in time."),Object(o.b)("p",null,"The start of processing is similar to the mechanism followed in ",Object(o.b)("em",{parentName:"p"},"update")," Stream. If metadata is updated, the Event processes and proceeds with executing the task. If the metadata is not updated within the desired time frame, the Event is discarded."),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},"Once scale processing starts, it first sets the Stream State to ",Object(o.b)("em",{parentName:"p"},"Scaling"),".")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},"Then creates new Stream Segments in Segment Store. The workflow is as follows:"),Object(o.b)("ol",{parentName:"li"},Object(o.b)("li",{parentName:"ol"},"After  successfully creating new segments, it creates a new epoch record in the\nmetadata store."),Object(o.b)("li",{parentName:"ol"},"The created new epoch record corresponds to a new epoch which contains the list of Stream Segments as they would appear post scale."),Object(o.b)("li",{parentName:"ol"},"Each new epoch creation also creates a new root epoch node under which the metadata for all transactions from that epoch resides."),Object(o.b)("li",{parentName:"ol"},"After creating requisite metadata records, scale workflow attempts to seal the old Stream segments in the Segment Store."))),Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},"After the old Stream Segments are sealed, we can safely mark the new epoch as the currently ",Object(o.b)("em",{parentName:"p"},"active")," epoch and reset state to ",Object(o.b)("em",{parentName:"p"},"Active"),"."))),Object(o.b)("h3",{id:"truncate-stream"},"Truncate Stream"),Object(o.b)("p",null,"Truncating a Stream follows a similar mechanism to update and has a temporary\nStream property for truncation that is used to supply input for truncate Stream."),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Once the truncate workflow process starts, the Stream State is set to ",Object(o.b)("em",{parentName:"li"},"Truncating"),"."),Object(o.b)("li",{parentName:"ul"},"Truncate workflow then looks at the requested ",Object(o.b)("inlineCode",{parentName:"li"},"StreamCut"),", and checks if it is greater than or equal to the existing truncation point, only then is it a valid input for truncation\nand the workflow commences."),Object(o.b)("li",{parentName:"ul"},"The truncation workflow takes the requested ",Object(o.b)("inlineCode",{parentName:"li"},"StreamCut")," and computes all Stream Segments that are to be deleted as part of this truncation request."),Object(o.b)("li",{parentName:"ul"},"Then calls into respective Segment Stores to delete identified Stream Segments. Post deletion, we call truncate on Stream Segments that are described in the ",Object(o.b)("inlineCode",{parentName:"li"},"StreamCut")," at the offsets as described in the ",Object(o.b)("inlineCode",{parentName:"li"},"Streamcut"),"."),Object(o.b)("li",{parentName:"ul"},"Following this, the truncation record is updated with the new\ntruncation point and deleted Stream Segments.  The state is reset to ",Object(o.b)("em",{parentName:"li"},"Active"),".")),Object(o.b)("h3",{id:"seal-stream"},"Seal Stream"),Object(o.b)("p",null,"Seal Stream can be requested via an explicit API call into Controller.\nIt first posts a seal Stream Event into request Stream."),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Once the Seal Stream process starts, the Stream State is set to ",Object(o.b)("em",{parentName:"li"},"Sealing"),"."),Object(o.b)("li",{parentName:"ul"},"If the event is picked and does not find the Stream to be in the desired state, it postpones the\nseal Stream processing by reposting it at the back of in-memory queue."),Object(o.b)("li",{parentName:"ul"},"Once the Stream is set to sealing state, all ",Object(o.b)("em",{parentName:"li"},"active")," Stream Segments for the\nStream are sealed by calling into Segment Store."),Object(o.b)("li",{parentName:"ul"},"After this, the Stream is marked as ",Object(o.b)("em",{parentName:"li"},"Sealed")," in the Stream metadata.")),Object(o.b)("h3",{id:"delete-stream"},"Delete Stream"),Object(o.b)("p",null,"Delete Stream can be requested via an explicit API call into Controller.\nThe request first verifies if the Stream is in ",Object(o.b)("em",{parentName:"p"},"Sealed")," state."),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Only sealed Streams can be deleted and an event to this effect is posted in the request\nStream."),Object(o.b)("li",{parentName:"ul"},"When the event is picked for processing, it verifies the Stream state again and then proceeds to delete all Stream Segments that belong to this Stream from its inception by calling into Segment Store."),Object(o.b)("li",{parentName:"ul"},"Once all Stream Segments are deleted successfully, the Stream metadata corresponding to\nthis Stream is cleaned up.")),Object(o.b)("h2",{id:"stream-policy-manager"},"Stream Policy Manager"),Object(o.b)("p",null,"As described earlier, there are two types of user-defined policies that Controller is responsible for enforcing, namely ",Object(o.b)("em",{parentName:"p"},"Automatic Scaling")," and ",Object(o.b)("em",{parentName:"p"},"Automatic Retention"),".\nThe Controller is not just the store for Stream policy but it actively enforces those user-defined policies for their Streams."),Object(o.b)("h3",{id:"scaling-infrastructure"},"Scaling Infrastructure"),Object(o.b)("p",null,"Scaling infrastructure is built in conjunction with Segment Stores. As the Controller creates new Stream Segments in Segment Stores, it passes user-defined scaling policies to Segment Stores. The Segment Store then\nmonitors traffic for the said Stream Segment and reports to Controller if some\nthresholds, as determined from policy, are breached. The Controller receives\nthese notifications via Events posted in dedicated internal Streams.\nThere are two types of traffic reports that can be received for\nsegments."),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"It identifies if a Stream Segment should be ",Object(o.b)("strong",{parentName:"li"},"scaled up")," (Split)."),Object(o.b)("li",{parentName:"ul"},"It identifies if a Stream Segment should be ",Object(o.b)("strong",{parentName:"li"},"scaled down")," (Merge).")),Object(o.b)("p",null,"For Stream Segments eligible for scale up, the Controller immediately posts the request for\nStream Segment scale up in the request Stream for Request Event Processor to\nprocess. However, for scale down, the Controller needs to wait for at least\ntwo neighboring Stream Segments to become eligible for scale down. For this\npurpose, it marks the Stream Segment as ",Object(o.b)("strong",{parentName:"p"},"cold")," in the metadata store. The Controller consolidates the neighboring Stream Segments that are marked as cold and posts a scale down the request for them.\nThe scale requests processing is then performed asynchronously on the\nRequest Event Processor.   "),Object(o.b)("h3",{id:"retention-infrastructure"},"Retention Infrastructure"),Object(o.b)("p",null,"The retention policy defines how much data should be retained for a\ngiven Stream. This can be defined as ",Object(o.b)("em",{parentName:"p"},"time-based")," or ",Object(o.b)("em",{parentName:"p"},"size-based"),". To apply\nthis policy, Controller periodically collects ",Object(o.b)("inlineCode",{parentName:"p"},"StreamCut"),"(s) for the Stream\nand opportunistically performs truncation on previously collected ",Object(o.b)("inlineCode",{parentName:"p"},"StreamCut"),"(s) if policy dictates it. Since this is a periodic background work that needs to be performed for all Streams that have a retention policy defined, there is an imperative need to fairly distribute this workload\nacross all available Controller instances. To achieve this we rely on\nbucketing Streams into predefined sets and distributing these sets\nacross Controller instances. This is done by using Zookeeper to store\nthis distribution. Each Controller instance, during bootstrap, attempts\nto acquire ownership of buckets. All Streams under a bucket are\nmonitored for retention opportunities by the owning Controller. At each\nperiod, Controller collects a new ",Object(o.b)("inlineCode",{parentName:"p"},"StreamCut")," and adds it to a\nretention set for the said Stream. Post this it looks for the candidate\n",Object(o.b)("inlineCode",{parentName:"p"},"StreamCut"),"(s) stored in retention set which are eligible for truncation\nbased on the defined retention policy. For example, in time-based\nretention, the latest ",Object(o.b)("inlineCode",{parentName:"p"},"StreamCut")," older than the specified retention period\nis chosen as the truncation point."),Object(o.b)("h2",{id:"transaction-manager"},"Transaction Manager"),Object(o.b)("p",null,"Another important role played by the Controller is that of the Transaction manager. It is responsible for the beginning and\nending Transactions. The Controller plays an active role in providing guarantees for Transactions from the time\nthey are created until the time they are committed or aborted. The Controller tracks each Transaction for their\nspecified timeouts, and automatically aborts the Transaction if the timeout exceeds.\nThe Controller is responsible for ensuring that the Transaction and a\npotential concurrent scale operation play well with each other and\nensure all promises made with respect to either are honored and\nenforced."),Object(o.b)("p",null,Object(o.b)("img",{alt:"Transaction Management",src:a(210).default})),Object(o.b)("p",null,Object(o.b)("em",{parentName:"p"},"Transaction Management Diagram")),Object(o.b)("p",null,"Client calls into Controller process to ",Object(o.b)("em",{parentName:"p"},"create, ping commit")," or ",Object(o.b)("em",{parentName:"p"},"abort\ntransactions"),". Each of these requests is received on Controller and handled by the Transaction Management module which\nimplements the business logic for processing each request."),Object(o.b)("h3",{id:"create-transaction"},"Create Transaction"),Object(o.b)("p",null,"Writers interact with Controller to create new Transactions. Controller Service passes the create transaction request to Transaction Management module."),Object(o.b)("p",null,"The create Transaction function in the module performs the following steps:"),Object(o.b)("ol",null,Object(o.b)("li",{parentName:"ol"},"Generates a unique UUID for the Transaction."),Object(o.b)("li",{parentName:"ol"},"It fetches the current ",Object(o.b)("em",{parentName:"li"},"active")," set of Stream Segments for the Stream from metadata store and its corresponding epoch identifier from the history."),Object(o.b)("li",{parentName:"ol"},"It creates a new Transaction record in the Zookeeper using the metadata store interface."),Object(o.b)("li",{parentName:"ol"},"It then requests Segment Store to create special Transaction Segments that are inherently linked to the parent ",Object(o.b)("em",{parentName:"li"},"active")," Stream Segments.")),Object(o.b)("p",null,"The Controller creates shadow Stream Segments for current ",Object(o.b)("em",{parentName:"p"},"active")," Segments by associating Transaction ID to compute unique shadow Stream Segment identifiers. The lifecycle of shadow Stream Segments are not linked to original Stream Segments and original Stream Segments can be sealed, truncated or deleted without affecting the lifecycle of shadow Stream Segment."),Object(o.b)("h3",{id:"commit-transaction"},"Commit Transaction"),Object(o.b)("p",null,"Upon receiving the request to commit a Transaction, Controller Service passes the request to Transaction Management module. This module first tries to mark the Transaction for commit in the Transaction specific metadata record via metadata store."),Object(o.b)("p",null,"Following this, it posts a commit Event in the internal Commit Stream. The commit event only captures the epoch in which the Transaction has to be committed. Commit Transaction workflow is implemented on commit Event processor and thereby processed asynchronously."),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"When commit workflow starts, it opportunistically collects all available Transactions that have been marked for commit in the given epoch and proceeds to commit them in order and one Transaction at a time."),Object(o.b)("li",{parentName:"ul"},"A Transaction commit entails merging the Transaction Segment into its parent Segment. This works perfectly in absence of scale.",Object(o.b)("ul",{parentName:"li"},Object(o.b)("li",{parentName:"ul"},"However, because of scaling of a Stream, some of the parent Segments for Transaction's shadow Stream Segments could have been sealed away."),Object(o.b)("li",{parentName:"ul"},"In such instance, when we attempt to commit a Transactions we may not have parent Segments in which Transaction Segments could be merged into."),Object(o.b)("li",{parentName:"ul"},"One approach to mitigate this could have been to prevent scaling operation while there were ongoing Transactions. However, this could stall scaling for an arbitrarily large period of time and would be detrimental. Instead, controller decouples scale and Transactions and allows either to occur concurrently without impacting workings of the other. This is achieved by using a scheme called ",Object(o.b)("strong",{parentName:"li"},"Rolling Transactions"),".")))),Object(o.b)("h4",{id:"rolling-transactions"},"Rolling Transactions"),Object(o.b)("p",null,"This is achieved by using a scheme (Rolling Transactions) where controller allows Transaction Segments to outlive their parent Segments and whenever their commits are issued, at a logical level controller elevates the Transaction Segments as first class Segments and includes them in a new epoch in the epoch time series of the Stream."),Object(o.b)("ol",null,Object(o.b)("li",{parentName:"ol"},"Transactions are created in an older epoch and when they are attempted to be committed, the latest epoch is sealed, Transactions are rolled over and included and then a duplicate of the latest epoch is created for Stream to restore its previous state before rolling of Transactions."),Object(o.b)("li",{parentName:"ol"},"This ensures that Transactions could be created at any time and then be committed at any time without interfering with any other Stream processing."),Object(o.b)("li",{parentName:"ol"},"The commit workflow on the controller guarantees that once started it will attempt to commit each of the identified Transactions with indefinite retries until they all succeed."),Object(o.b)("li",{parentName:"ol"},"Once a Transaction is committed successfully, the record for the Transaction is removed from under its epoch root.")),Object(o.b)("h3",{id:"abort-transaction"},"Abort Transaction"),Object(o.b)("p",null,"Abort, like commit, can be requested explicitly by the application. However, abort can also be initiated automatically if the Transaction\u2019s timeout elapses."),Object(o.b)("ol",null,Object(o.b)("li",{parentName:"ol"},"The Controller tracks the timeout for each and every Transaction in the system and whenever timeout elapses, or upon explicit user request, Transaction Management module marks the Transaction for abort in its respective metadata."),Object(o.b)("li",{parentName:"ol"},"After this, the Event is picked for processing by abort Event Processor and the Transactions abort is immediately attempted."),Object(o.b)("li",{parentName:"ol"},"There is no ordering requirement for abort Transaction and hence it is\nperformed concurrently and across Streams.")),Object(o.b)("h3",{id:"ping-transaction"},"Ping Transaction"),Object(o.b)("p",null,"Since Controller has no visibility into data path with respect to data\nbeing written to segments in a Transaction, Controller is unaware if a\nTransaction is being actively worked upon or not and if the timeout\nelapses it may attempt to abort the Transaction. To enable applications\nto control the destiny of a Transaction, Controller exposes an API to\nallow applications to renew Transaction timeout period. This mechanism\nis called ping and whenever application pings a Transaction, Controller\nresets its timer for respective transaction."),Object(o.b)("h3",{id:"transaction-timeout-management"},"Transaction Timeout Management"),Object(o.b)("p",null,"Controllers track each Transaction for their timeouts. This is\nimplemented as ",Object(o.b)("em",{parentName:"p"},"timer wheel service"),". Each Transaction, upon creation gets\nregistered into the timer service on the Controller where it is created.\nSubsequent pings for the Transaction could be received on different\nController instances and timer management is transferred to the latest\nController instance based on ownership mechanism implemented via\nZookeeper. Upon timeout expiry, an automatic abort is attempted and if\nit is able to successfully set Transaction status to abort, the abort\nworkflow is initiated."),Object(o.b)("p",null,"Each Transaction that a Controller is monitoring for timeouts is added\nto this processes index. If such a Controller instance fails or crashes,\nother Controller instances will receive node failed notification and\nattempt to sweep all outstanding Transactions from the failed instance\nand monitor their timeouts from that point onward."),Object(o.b)("h2",{id:"segment-container-to-host-mapping"},"Segment Container to Host Mapping"),Object(o.b)("p",null,"The Controller is also responsible for the assignment of Segment Containers to\nSegment Store nodes. The responsibility of maintaining this mapping\nbefalls a single Controller instance that is chosen via a leader\nelection using Zookeeper. This leader Controller monitors lifecycle of\nSegment Store nodes as they are added to/removed from the cluster and\nperforms redistribution of Segment Containers across available Segment\nStore nodes. This distribution mapping is stored in a dedicated znode.\nEach Segment Store periodically polls this znode to look for changes and\nif changes are found, it shuts down and relinquishes containers it no\nlonger owns and attempts to acquire ownership of containers that are\nassigned to it."),Object(o.b)("p",null,"The details about implementation, especially with respect to how the metadata is stored and managed is already discussed in the section ",Object(o.b)("a",{parentName:"p",href:"#controller-cluster-listener"},"Cluster Listener"),"."),Object(o.b)("h1",{id:"resources"},"Resources"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"https://pravega.io/"},"Pravega")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",{parentName:"li",href:"https://github.com/pravega/pravega/tree/master/controller"},"Code"))))}m.isMDXComponent=!0}}]);