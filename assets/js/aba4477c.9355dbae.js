(window.webpackJsonp=window.webpackJsonp||[]).push([[59],{130:function(e,t,a){"use strict";a.r(t),a.d(t,"frontMatter",(function(){return i})),a.d(t,"metadata",(function(){return o})),a.d(t,"toc",(function(){return s})),a.d(t,"default",(function(){return c}));var n=a(3),l=a(7),r=(a(0),a(160)),i={title:"Pravega Flink Tools"},o={unversionedId:"sdp/edge/pravega-flink-tools",id:"sdp/edge/pravega-flink-tools",isDocsHomePage:!1,title:"Pravega Flink Tools",description:"Pravega Flink Tools is a collection of Apache Flink applications for working with Pravega streams.",source:"@site/docs/sdp/edge/pravega-flink-tools.md",slug:"/sdp/edge/pravega-flink-tools",permalink:"/docs/docs/sdp/edge/pravega-flink-tools",editUrl:null,version:"current",sidebar:"mainSidebar",previous:{title:"Pravega Sensor Collector",permalink:"/docs/docs/sdp/edge/pravega-sensor-collector"},next:{title:"Pravega Overview",permalink:"/docs/docs/pravega/overview"}},s=[{value:"Prerequisites",id:"prerequisites",children:[]},{value:"Stream-to-Console: Continuously show the contents of a Pravega stream in a human-readable log file",id:"stream-to-console-continuously-show-the-contents-of-a-pravega-stream-in-a-human-readable-log-file",children:[{value:"Overview",id:"overview",children:[]},{value:"Run Locally with Gradle",id:"run-locally-with-gradle",children:[]}]},{value:"Stream-to-File: Continuously copying a Pravega stream to text files",id:"stream-to-file-continuously-copying-a-pravega-stream-to-text-files",children:[{value:"Overview",id:"overview-1",children:[]},{value:"Deploy to SDP using the SDP UI",id:"deploy-to-sdp-using-the-sdp-ui",children:[]},{value:"Deploy to SDP using Helm",id:"deploy-to-sdp-using-helm",children:[]}]},{value:"Stream-to-Parquet-File: Continuously copying a Pravega stream to Parquet files",id:"stream-to-parquet-file-continuously-copying-a-pravega-stream-to-parquet-files",children:[{value:"Overview",id:"overview-2",children:[]},{value:"Flattening records",id:"flattening-records",children:[]},{value:"Deploy to SDP",id:"deploy-to-sdp",children:[]},{value:"How to view Parquet files",id:"how-to-view-parquet-files",children:[]}]},{value:"Stream-to-CSV-File: Continuously copying a Pravega stream to CSV files",id:"stream-to-csv-file-continuously-copying-a-pravega-stream-to-csv-files",children:[{value:"Overview",id:"overview-3",children:[]},{value:"Run Locally with Gradle",id:"run-locally-with-gradle-1",children:[]}]},{value:"Writing to an NFS volume",id:"writing-to-an-nfs-volume",children:[]},{value:"Stream-to-Stream: Continuously copying a Pravega stream to another Pravega stream",id:"stream-to-stream-continuously-copying-a-pravega-stream-to-another-pravega-stream",children:[{value:"Overview",id:"overview-4",children:[]},{value:"Deploy to SDP",id:"deploy-to-sdp-1",children:[]}]},{value:"Sample data generator",id:"sample-data-generator",children:[{value:"Overview",id:"overview-5",children:[]},{value:"Run Locally with Gradle",id:"run-locally-with-gradle-2",children:[]},{value:"Deploy to SDP using the SDP UI",id:"deploy-to-sdp-using-the-sdp-ui-1",children:[]},{value:"Deploy to SDP using Helm",id:"deploy-to-sdp-using-helm-1",children:[]}]},{value:"Deduplication of events",id:"deduplication-of-events",children:[]},{value:"Enabling deduplication",id:"enabling-deduplication",children:[]},{value:"Deploying to SDP using Helm without Internet access",id:"deploying-to-sdp-using-helm-without-internet-access",children:[{value:"Build the installation archive",id:"build-the-installation-archive",children:[]},{value:"Deploy using the installation archive",id:"deploy-using-the-installation-archive",children:[]}]},{value:"Github Release Procedure",id:"github-release-procedure",children:[]},{value:"References",id:"references",children:[]},{value:"See Also",id:"see-also",children:[]},{value:"About",id:"about",children:[]}],p={toc:s};function c(e){var t=e.components,a=Object(l.a)(e,["components"]);return Object(r.b)("wrapper",Object(n.a)({},p,a,{components:t,mdxType:"MDXLayout"}),Object(r.b)("p",null,"Pravega Flink Tools is a collection of Apache Flink applications for working with Pravega streams."),Object(r.b)("p",null,"It provides the following Flink jobs:"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},Object(r.b)("strong",{parentName:"li"},"stream-to-file"),": Continuously copy a Pravega stream to text files on S3, HDFS, or any other Flink-supported file system"),Object(r.b)("li",{parentName:"ul"},Object(r.b)("strong",{parentName:"li"},"stream-to-parquet-file"),": Continuously copy a Pravega stream to Parquet files on S3, HDFS, or any other Flink-supported file system"),Object(r.b)("li",{parentName:"ul"},Object(r.b)("strong",{parentName:"li"},"stream-to-csv-file"),": Continuously copy a Pravega stream to CSV files on S3, HDFS, or any other Flink-supported file system"),Object(r.b)("li",{parentName:"ul"},Object(r.b)("strong",{parentName:"li"},"stream-to-stream"),": Continuously copy a Pravega stream to another Pravega stream, even on a different Pravega cluster"),Object(r.b)("li",{parentName:"ul"},Object(r.b)("strong",{parentName:"li"},"stream-to-console"),": Continuously show the contents of a Pravega stream in a human-readable log file"),Object(r.b)("li",{parentName:"ul"},Object(r.b)("strong",{parentName:"li"},"sample-data-generator"),": Continuously write synthetic data to Pravega for testing")),Object(r.b)("p",null,"Each job uses ",Object(r.b)("a",{parentName:"p",href:"https://ci.apache.org/projects/flink/flink-docs-stable/ops/state/checkpoints.html"},"Flink checkpoints")," to provide exactly-once guarantees, ensuring that events\nare never missed nor duplicated.\nThey automatically recover from failures and resume where they left off.\nThey can use parallelism for high-volume streams with multiple segments."),Object(r.b)("p",null,"Pravega Flink Tools is 100% open source and community-driven. The source code is available on ",Object(r.b)("a",{parentName:"p",href:"https://raw.githubusercontent.com/pravega/flink-tools"},"GitHub"),"."),Object(r.b)("h2",{id:"prerequisites"},"Prerequisites"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},"Java JDK 8.x.\nOn Ubuntu, this can be installed with:"),Object(r.b)("pre",{parentName:"li"},Object(r.b)("code",{parentName:"pre",className:"language-shell"},"sudo apt-get install openjdk-8-jdk\n"))),Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},"A ",Object(r.b)("a",{parentName:"p",href:"http://pravega.io"},"Pravega")," installation")),Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},"The deployment scripts in this project are designed to work with\n",Object(r.b)("strong",{parentName:"p"},"Dell EMC Streaming Data Platform")," (SDP).\nThese Flink tools may also be used in other Flink installations,\nincluding open-source, although the exact\ndeployment methods depend on your environment and are not documented here."))),Object(r.b)("h2",{id:"stream-to-console-continuously-show-the-contents-of-a-pravega-stream-in-a-human-readable-log-file"},"Stream-to-Console: Continuously show the contents of a Pravega stream in a human-readable log file"),Object(r.b)("h3",{id:"overview"},"Overview"),Object(r.b)("p",null,"This Flink job will continuously print the events in a Pravega stream.\nEvents must consist of UTF-8 strings, such as JSON or CSV."),Object(r.b)("h3",{id:"run-locally-with-gradle"},"Run Locally with Gradle"),Object(r.b)("p",null,"Use the command below to run the Flink job directly with Gradle.\nAll job parameters must be specified within the Gradle ",Object(r.b)("inlineCode",{parentName:"p"},"--args")," argument."),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-shell"},'./gradlew -PmainClass=io.pravega.flinktools.StreamToConsoleJob \\\nflink-tools:run \\\n--args="\\\n--input-stream examples/sample1 \\\n--input-startAtTail true \\\n"\n')),Object(r.b)("p",null,"The contents of the Pravega stream will be displayed in the console along\nwith a variety of informational messages."),Object(r.b)("h2",{id:"stream-to-file-continuously-copying-a-pravega-stream-to-text-files"},"Stream-to-File: Continuously copying a Pravega stream to text files"),Object(r.b)("h3",{id:"overview-1"},"Overview"),Object(r.b)("p",null,"This Flink job will continuously copy a Pravega stream to a set of text files\non S3, HDFS, NFS, or any other Flink-supported file system.\nIt uses Flink checkpoints to provide exactly-once guarantees, ensuring that events\nare never missed nor duplicated.\nIt automatically recovers from failures and resumes where it left off.\nIt can use parallelism for high-volume streams with multiple segments."),Object(r.b)("p",null,"By default, it writes a new file every 1 minute.\nFiles are written using the following directory structure."),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre"},"sample1/2020-05-10--18/part-0-0\nsample1/2020-05-10--18/part-0-1\nsample1/2020-05-10--18/part-0-2\n...\nsample1/2020-05-10--18/part-0-59\nsample1/2020-05-10--19/part-0-60\nsample1/2020-05-10--19/part-0-61\n")),Object(r.b)("p",null,"For simplicity, the current implementation assumes that events are UTF-8 strings such as CSV or JSON.\nWhen written to text files, each event will be followed by a new line.\nNo other transformation is performed by this job.\nFor binary events, you will need to customize the Flink job with the appropriate serialization classes."),Object(r.b)("p",null,"Flink offers many options for customizing the behavior when writing files.\nRefer to ",Object(r.b)("a",{parentName:"p",href:"https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/connectors/streamfile_sink.html"},"Steaming File Sink"),"\nfor details."),Object(r.b)("h3",{id:"deploy-to-sdp-using-the-sdp-ui"},"Deploy to SDP using the SDP UI"),Object(r.b)("p",null,"Below shows how to deploy this Flink job using the SDP UI.\nIf you would rather use a more automated deployment method, skip to the next section."),Object(r.b)("ol",null,Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"Build the JAR file."),Object(r.b)("pre",{parentName:"li"},Object(r.b)("code",{parentName:"pre",className:"language-shell"},"./gradlew clean shadowJar\n"))),Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"Upload the artifact:"),Object(r.b)("ul",{parentName:"li"},Object(r.b)("li",{parentName:"ul"},"group: io.pravega"),Object(r.b)("li",{parentName:"ul"},"artifact: flink-tools"),Object(r.b)("li",{parentName:"ul"},"version: 0.2.0"),Object(r.b)("li",{parentName:"ul"},"file: flink-tools/build/libs/pravega-flink-tools-0.2.0.jar"))),Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"Create Flink Cluster."),Object(r.b)("ul",{parentName:"li"},Object(r.b)("li",{parentName:"ul"},"Name: stream-to-file"),Object(r.b)("li",{parentName:"ul"},"Flink Image: 1.10.2-2.12-hadoop2.8.3 (1.10.2-2.12)"),Object(r.b)("li",{parentName:"ul"},"Replicas: 1"),Object(r.b)("li",{parentName:"ul"},"Task Slots: 1"))),Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"Create New App."),Object(r.b)("ul",{parentName:"li"},Object(r.b)("li",{parentName:"ul"},"Name: stream-to-file"),Object(r.b)("li",{parentName:"ul"},"Artifact: io.pravega:fliink-tools:0.2.0"),Object(r.b)("li",{parentName:"ul"},"Main Class: io.pravega.flinktools.StreamToFileJob"),Object(r.b)("li",{parentName:"ul"},"Cluster Selectors: name: stream-to-file"),Object(r.b)("li",{parentName:"ul"},"Parallelism: 1"),Object(r.b)("li",{parentName:"ul"},"Flink Version: 1.10.2-2.12"),Object(r.b)("li",{parentName:"ul"},"Add Parameters:",Object(r.b)("ul",{parentName:"li"},Object(r.b)("li",{parentName:"ul"},"output: hdfs://hadoop-hadoop-hdfs-nn.examples.svc.cluster.local:9000/tmp/sample1"),Object(r.b)("li",{parentName:"ul"},"scope: examples (This should match your SDP project name.)"))),Object(r.b)("li",{parentName:"ul"},"Add Stream:",Object(r.b)("ul",{parentName:"li"},Object(r.b)("li",{parentName:"ul"},"input-stream: sample1")))))),Object(r.b)("h3",{id:"deploy-to-sdp-using-helm"},"Deploy to SDP using Helm"),Object(r.b)("ol",null,Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"If you will be using HDFS, you must install the Flink cluster image that includes the Hadoop client library."),Object(r.b)("pre",{parentName:"li"},Object(r.b)("code",{parentName:"pre",className:"language-shell"},"scripts/flink-image-install.sh\n"))),Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"Copy the file ",Object(r.b)("inlineCode",{parentName:"p"},"scripts/env-sample.sh")," to ",Object(r.b)("inlineCode",{parentName:"p"},"scripts/env-local.sh"),"\nor any other destination.\nThis script will contain parameters for your environment.\nEdit the file as follows."),Object(r.b)("p",{parentName:"li"},"a. Enter your Kubernetes namespace that contains your Pravega stream (NAMESPACE).\nThis is the name of your analytics project."),Object(r.b)("p",{parentName:"li"},"Example file ",Object(r.b)("inlineCode",{parentName:"p"},"scripts/env-local.sh"),":"),Object(r.b)("pre",{parentName:"li"},Object(r.b)("code",{parentName:"pre",className:"language-shell"},"export NAMESPACE=examples\n"))),Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"Copy the sample values file from ",Object(r.b)("inlineCode",{parentName:"p"},"values/samples/sample1-stream-to-aws-s3-job.yaml")," or\n",Object(r.b)("inlineCode",{parentName:"p"},"values/samples/sample1-stream-to-hdfs-job.yaml")," to\n",Object(r.b)("inlineCode",{parentName:"p"},"values/local/my-stream-to-file-job.yaml")," or any other destination.\nYou may name this file anything, but you must use alphanumeric characters and dashes only.")),Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"Edit this file to use your Pravega stream name and output directory.\nYou can also change the checkpoint interval, which is how often events\nwill be written to the files.")),Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"Launch the Flink job using Helm."),Object(r.b)("pre",{parentName:"li"},Object(r.b)("code",{parentName:"pre",className:"language-shell"},"scripts/jobs/stream-to-file-job.sh values/local/my-stream-to-file-job.yaml\n"))),Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"To copy additional streams, repeat steps 3 to 5.")),Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"To stop the job and delete all associated state:"),Object(r.b)("pre",{parentName:"li"},Object(r.b)("code",{parentName:"pre",className:"language-shell"},"helm del my-stream-to-file-job -n ${NAMESPACE}\n")))),Object(r.b)("h2",{id:"stream-to-parquet-file-continuously-copying-a-pravega-stream-to-parquet-files"},"Stream-to-Parquet-File: Continuously copying a Pravega stream to Parquet files"),Object(r.b)("h3",{id:"overview-2"},"Overview"),Object(r.b)("p",null,"This Flink job will continuously copy a Paravega stream to a set of\n",Object(r.b)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Apache_Parquet"},"Apache Parquet")," files\non S3, HDFS, NFS, or any other Flink-supported file system."),Object(r.b)("p",null,"Apache Parquet is a column-oriented data storage format of the Apache Hadoop ecosystem.\nIt provides efficient data compression and encoding schemes with enhanced performance to handle complex data in bulk."),Object(r.b)("p",null,"This Flink job uses Flink checkpoints to provide exactly-once guarantees, ensuring that events\nare never missed nor duplicated.\nIt automatically recovers from failures and resumes where it left off.\nIt can use parallelism for high-volume streams with multiple segments."),Object(r.b)("p",null,"Input events must be in JSON format.\nTo ensure that JSON events can be reliable converted to Parquet, you must specify the\n",Object(r.b)("a",{parentName:"p",href:"http://avro.apache.org/docs/1.8.2/spec.html"},"Apache Avro schema")," that corresponds to the JSON events."),Object(r.b)("p",null,"By default, it will compress the data in Snappy format and write a new file every 1 minute.\nFiles are written using the following directory structure."),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre"},"sample1/2020-05-10--18/part-0-0\nsample1/2020-05-10--18/part-0-1\nsample1/2020-05-10--18/part-0-2\n...\nsample1/2020-05-10--18/part-0-59\nsample1/2020-05-10--19/part-0-60\nsample1/2020-05-10--19/part-0-61\n")),Object(r.b)("h3",{id:"flattening-records"},"Flattening records"),Object(r.b)("p",null,"When writing to Parquet files, the input JSON events can be optionally transformed\nusing a flatten operation."),Object(r.b)("p",null,"When enabled, input JSON events in this format"),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-json"},'{\n"RemoteAddr":"gw3001",\n"Timestamp":[1597883617606,1597883617616,1597883617626],\n"X":[0.0,0.1,0.2],\n"Y":[0.0,-0.1,-0.2],\n"Z":[9.9,9.8,9.7]\n}\n')),Object(r.b)("p",null,"will be written with this structure."),Object(r.b)("table",null,Object(r.b)("thead",{parentName:"table"},Object(r.b)("tr",{parentName:"thead"},Object(r.b)("th",{parentName:"tr",align:null},"RemoteAddr"),Object(r.b)("th",{parentName:"tr",align:null},"Timestamp"),Object(r.b)("th",{parentName:"tr",align:null},"X"),Object(r.b)("th",{parentName:"tr",align:null},"Y"),Object(r.b)("th",{parentName:"tr",align:null},"Z"))),Object(r.b)("tbody",{parentName:"table"},Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",{parentName:"tr",align:null},"gw3001"),Object(r.b)("td",{parentName:"tr",align:null},"1597883617606"),Object(r.b)("td",{parentName:"tr",align:null},"0.0"),Object(r.b)("td",{parentName:"tr",align:null},"0.0"),Object(r.b)("td",{parentName:"tr",align:null},"9.9")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",{parentName:"tr",align:null},"gw3001"),Object(r.b)("td",{parentName:"tr",align:null},"1597883617616"),Object(r.b)("td",{parentName:"tr",align:null},"0.1"),Object(r.b)("td",{parentName:"tr",align:null},"-0.1"),Object(r.b)("td",{parentName:"tr",align:null},"9.8")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",{parentName:"tr",align:null},"gw3001"),Object(r.b)("td",{parentName:"tr",align:null},"1597883617626"),Object(r.b)("td",{parentName:"tr",align:null},"0.2"),Object(r.b)("td",{parentName:"tr",align:null},"-0.2"),Object(r.b)("td",{parentName:"tr",align:null},"9.7")))),Object(r.b)("p",null,"All fields containing arrays must have the same number of elements.\nAll non-array fields will be duplicated on each record."),Object(r.b)("p",null,"This flatten transformation can be enabled by setting the ",Object(r.b)("inlineCode",{parentName:"p"},"flatten")," parameter to ",Object(r.b)("inlineCode",{parentName:"p"},"true"),"."),Object(r.b)("h3",{id:"deploy-to-sdp"},"Deploy to SDP"),Object(r.b)("p",null,"Refer to the method described in the Stream-to-File section.\nUse the script ",Object(r.b)("inlineCode",{parentName:"p"},"scripts/jobs/stream-to-parquet-file-job.sh")," and a values file similar to\n",Object(r.b)("inlineCode",{parentName:"p"},"values/samples/sample1-stream-to-parquet-hdfs-job.yaml"),"."),Object(r.b)("h3",{id:"how-to-view-parquet-files"},"How to view Parquet files"),Object(r.b)("p",null,"If the Parquet file is located on a standard Linux file system (including NFS),\nyou can use a command similar to the following to view the content."),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-shell"},"scripts/parquet-tools.sh cat /tmp/sample1-parquet/2020-08-19--03/part-0-887\n")),Object(r.b)("p",null,"If the Parquet file is located on an HDFS cluster in Kubernetes,\nyou can use commands similar to the following to view the content."),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-shell"},"scripts/hadoop-bash.sh\nroot@hadoop-8c428aa0-76c0-4f42-8bea-2fc1e8300f78:~#\nwget https://repo1.maven.org/maven2/org/apache/parquet/parquet-tools/1.11.1/parquet-tools-1.11.1.jar\nhadoop jar parquet-tools-1.11.1.jar cat hdfs://hadoop-hadoop-hdfs-nn.examples.svc.cluster.local:9000/tmp/sample1-parquet/2020-08-19--03/part-0-887\n")),Object(r.b)("h2",{id:"stream-to-csv-file-continuously-copying-a-pravega-stream-to-csv-files"},"Stream-to-CSV-File: Continuously copying a Pravega stream to CSV files"),Object(r.b)("h3",{id:"overview-3"},"Overview"),Object(r.b)("p",null,"This Flink job will continuously copy a Paravega stream to a set of\ncomma-separated value (CSV) files on S3, HDFS, NFS, or any other Flink-supported file system."),Object(r.b)("p",null,"Like Stream-to-Parquet-File, input events must be in JSON format\nand the corresponding Avro schema must be specified.\nAdditionally, events can be deduplicated and flattened if needed."),Object(r.b)("h3",{id:"run-locally-with-gradle-1"},"Run Locally with Gradle"),Object(r.b)("p",null,"Use the command below to run the Flink job directly with Gradle.\nAll job parameters must be specified within the Gradle ",Object(r.b)("inlineCode",{parentName:"p"},"--args")," argument."),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-shell"},'./gradlew -PmainClass=io.pravega.flinktools.StreamToCsvFileJob \\\nflink-tools:run \\\n--args="\\\n--input-stream examples/sample1 \\\n--input-startAtTail false \\\n--output /tmp/sample1.csv \\\n--avroSchemaFile ../test/SampleEvent.avsc \\\n--flatten false \\\n--logOutputRecords true \\\n"\n')),Object(r.b)("h2",{id:"writing-to-an-nfs-volume"},"Writing to an NFS volume"),Object(r.b)("p",null,"Use this procedure to configure the Flink Stream to File job to write to any Kubernetes Persistent Volume, such as a remote NFS volume."),Object(r.b)("ol",null,Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"Create a Persistent Volume and a Persistent Volume Claim."),Object(r.b)("p",{parentName:"li"},"a. Edit ",Object(r.b)("inlineCode",{parentName:"p"},"values/nfs-samples/external-nfs-pv.yaml")," and\n",Object(r.b)("inlineCode",{parentName:"p"},"values/nfs-samples/external-nfs-pv.yaml"),"."),Object(r.b)("p",{parentName:"li"},"b. Create objects."),Object(r.b)("pre",{parentName:"li"},Object(r.b)("code",{parentName:"pre",className:"language-shell"},"export NAMESPACE=examples\nkubectl create -f values/nfs-samples/external-nfs-pv.yaml\nkubectl create -f values/nfs-samples/external-nfs-pvc.yaml -n ${NAMESPACE}\n"))),Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"Configure extra NFS mount in ",Object(r.b)("inlineCode",{parentName:"p"},"values/samples/sample1-stream-to-nfs-job.yaml")),Object(r.b)("pre",{parentName:"li"},Object(r.b)("code",{parentName:"pre"},"jobManager:\n  volumeMounts:\n    - mountPath: /mnt/nfs\n      name: extra-volume\ntaskManager:\n  volumeMounts:\n    - mountPath: /mnt/examples-sample\n      name: extra-volume\n"))),Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"Configure mount path in values/samples/sample1-stream-to-nfs-job.yaml"),Object(r.b)("pre",{parentName:"li"},Object(r.b)("code",{parentName:"pre"},'output: "/mnt/nfs"\n'))),Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"Deploy Flink job for NFS."),Object(r.b)("pre",{parentName:"li"},Object(r.b)("code",{parentName:"pre",className:"language-shell"},"scripts/jobs/stream-to-file-job.sh values/samples/sample1-stream-to-nfs-job.yaml\n")))),Object(r.b)("h2",{id:"stream-to-stream-continuously-copying-a-pravega-stream-to-another-pravega-stream"},"Stream-to-Stream: Continuously copying a Pravega stream to another Pravega stream"),Object(r.b)("h3",{id:"overview-4"},"Overview"),Object(r.b)("p",null,"This Flink job will continuously copy a Pravega stream to another Pravega stream.\nIt uses Flink checkpoints to provide exactly-once guarantees, ensuring that events\nare never missed nor duplicated.\nIt automatically recovers from failures and resumes where it left off.\nIt can use parallelism for high-volume streams with multiple segments."),Object(r.b)("h3",{id:"deploy-to-sdp-1"},"Deploy to SDP"),Object(r.b)("p",null,"Refer to the method described in the Stream-to-File section."),Object(r.b)("h2",{id:"sample-data-generator"},"Sample data generator"),Object(r.b)("h3",{id:"overview-5"},"Overview"),Object(r.b)("p",null,"This Flink job continuously generates synthetic JSON events and writes them to a Pravega stream.\nIt can be used for testing the other applications.\nThe event size and event rate can be specified as parameters."),Object(r.b)("p",null,"Events are in the format shown below."),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-json"},'{"sensorId":0,"eventNumber":42,"timestamp":1591294714504,"timestampStr":"2020-06-04 18:18:34.504","data":"xxxxx..."}\n')),Object(r.b)("h3",{id:"run-locally-with-gradle-2"},"Run Locally with Gradle"),Object(r.b)("p",null,"Use the command below to run the Flink job directly with Gradle.\nAll job parameters must be specified within the Gradle ",Object(r.b)("inlineCode",{parentName:"p"},"--args")," argument."),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-shell"},'./gradlew -PmainClass=io.pravega.flinktools.SampleDataGeneratorJob \\\nflink-tools:run \\\n--args="\\\n--output-stream examples/sample1 \\\n"\n')),Object(r.b)("h3",{id:"deploy-to-sdp-using-the-sdp-ui-1"},"Deploy to SDP using the SDP UI"),Object(r.b)("p",null,"Below shows how to deploy this Flink job using the SDP UI."),Object(r.b)("ol",null,Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"Build the JAR file."),Object(r.b)("pre",{parentName:"li"},Object(r.b)("code",{parentName:"pre",className:"language-shell"},"./gradlew clean shadowJar\n"))),Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"Upload the artifact:"),Object(r.b)("ul",{parentName:"li"},Object(r.b)("li",{parentName:"ul"},"group: io.pravega"),Object(r.b)("li",{parentName:"ul"},"artifact: flink-tools"),Object(r.b)("li",{parentName:"ul"},"version: 0.2.0"),Object(r.b)("li",{parentName:"ul"},"file: flink-tools/build/libs/pravega-flink-tools-0.2.0.jar"))),Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"Create Flink Cluster."),Object(r.b)("ul",{parentName:"li"},Object(r.b)("li",{parentName:"ul"},"Name: sample-data-generator-job"),Object(r.b)("li",{parentName:"ul"},"Label:",Object(r.b)("ul",{parentName:"li"},Object(r.b)("li",{parentName:"ul"},"key: name"),Object(r.b)("li",{parentName:"ul"},"value: sample-data-generator-job"))),Object(r.b)("li",{parentName:"ul"},"Flink Image: 1.10.2-2.12 (1.10.2-2.12)"),Object(r.b)("li",{parentName:"ul"},"Replicas: 1"),Object(r.b)("li",{parentName:"ul"},"Task Slots: 1"))),Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"Create New App."),Object(r.b)("ul",{parentName:"li"},Object(r.b)("li",{parentName:"ul"},"Name: sample-data-generator-job"),Object(r.b)("li",{parentName:"ul"},"Artifact: io.pravega:fliink-tools:0.2.0"),Object(r.b)("li",{parentName:"ul"},"Main Class: io.pravega.flinktools.SampleDataGeneratorJob"),Object(r.b)("li",{parentName:"ul"},"Cluster Selectors: name: sample-data-generator-job"),Object(r.b)("li",{parentName:"ul"},"Parallelism: 1"),Object(r.b)("li",{parentName:"ul"},"Flink Version: 1.10.2-2.12"),Object(r.b)("li",{parentName:"ul"},"Add Parameters:",Object(r.b)("ul",{parentName:"li"},Object(r.b)("li",{parentName:"ul"},"scope: examples (This should match your SDP project name.)"))),Object(r.b)("li",{parentName:"ul"},"Add Stream:",Object(r.b)("ul",{parentName:"li"},Object(r.b)("li",{parentName:"ul"},"output-stream: sample1 (Select the stream to write to.)")))))),Object(r.b)("h3",{id:"deploy-to-sdp-using-helm-1"},"Deploy to SDP using Helm"),Object(r.b)("p",null,"Refer to the method described in the Stream-to-file section."),Object(r.b)("h2",{id:"deduplication-of-events"},"Deduplication of events"),Object(r.b)("p",null,"Although Pravega provides exactly-once semantics, some event sources may not fully utilize\nthese features and these may produce duplicate events in some failure cases.\nThe jobs in Pravega Flink Tools can use the stateful functionality of Flink to\nremove duplicates when writing to the output stream or file."),Object(r.b)("p",null,"This functionality requires the following:"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},"The events must be JSON objects such as\n",Object(r.b)("inlineCode",{parentName:"p"},'{"key1": 1, "counter": 10, "value1": 100}'),".")),Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},'The events must have one or more key columns, such as "key1" in the above example.\nEach key may be any JSON data type, including strings, numbers, arrays, and objects.')),Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},'The event must have a counter column, such as "counter" in the above example.\nIt must be a numeric data type. A 64-bit long integer is recommended.')),Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},"For each unique key, the counter ",Object(r.b)("em",{parentName:"p"},"must")," be ascending.\nIf the counter decreases or repeats, the event is considered a duplicate and it is logged and dropped."))),Object(r.b)("p",null,"Beware of using a timestamp for the counter.\nA clock going backwards due to a manual time correction or a leap second may result in data being dropped.\nEvents produced faster than the clock resolution may also be dropped."),Object(r.b)("h2",{id:"enabling-deduplication"},"Enabling deduplication"),Object(r.b)("ol",null,Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"Set the parameter ",Object(r.b)("inlineCode",{parentName:"p"},"keyFieldNames")," to a list of JSON field names for the keys, separated by commas.")),Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"Set the parameter ",Object(r.b)("inlineCode",{parentName:"p"},"counterFieldName")," to the JSON field name for the counter."))),Object(r.b)("p",null,"If either of these parameters is empty, deduplication will be disabled, which is the default behavior."),Object(r.b)("p",null,"When deduplication is enabled, any errors when parsing the JSON or accessing the keys or counter\nwill be logged and the event will be discarded."),Object(r.b)("h2",{id:"deploying-to-sdp-using-helm-without-internet-access"},"Deploying to SDP using Helm without Internet access"),Object(r.b)("p",null,"This procedure can be used to automate the deployment of the Flink jobs on a system without Internet access."),Object(r.b)("h3",{id:"build-the-installation-archive"},"Build the installation archive"),Object(r.b)("p",null,"This must be executed on a build host that has Internet access.\nThis will download all dependencies and create a single archive that can be copied to an offline SDP system."),Object(r.b)("ol",null,Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"On the build host, build the installation archive."),Object(r.b)("pre",{parentName:"li"},Object(r.b)("code",{parentName:"pre",className:"language-shell"},"user@build-host:~$\ngit clone https://github.com/pravega/flink-tools\ncd flink-tools\nscripts/build-installer.sh\n")),Object(r.b)("p",{parentName:"li"},"This will create the installation archive ",Object(r.b)("inlineCode",{parentName:"p"},"build/installer/flink-tools-${APP_VERSION}.tgz"),"."))),Object(r.b)("h3",{id:"deploy-using-the-installation-archive"},"Deploy using the installation archive"),Object(r.b)("ol",null,Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"The only prerequisite on the SDP system is Java 8.x.\nOn Ubuntu, this can be installed with:"),Object(r.b)("pre",{parentName:"li"},Object(r.b)("code",{parentName:"pre",className:"language-shell"},"sudo apt-get install openjdk-8-jdk\n"))),Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"Copy the installation archive to the SDP system, then extract it."),Object(r.b)("pre",{parentName:"li"},Object(r.b)("code",{parentName:"pre",className:"language-shell"},"user@sdp-host:~/desdp$\ntar -xzf flink-tools-*.tgz\ncd flink-tools\n"))),Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"Continue with the procedure in the section ",Object(r.b)("a",{parentName:"p",href:"#deploy-to-sdp-using-helm"},"Deploy to SDP using Helm"),"."))),Object(r.b)("h2",{id:"github-release-procedure"},"Github Release Procedure"),Object(r.b)("p",null,"Follow these steps to release a new version of Flink Tools to Github Releases."),Object(r.b)("ol",null,Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"Increment APP_VERSION in ",Object(r.b)("inlineCode",{parentName:"p"},"scripts/env.sh"),".")),Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"Run the following commands."),Object(r.b)("pre",{parentName:"li"},Object(r.b)("code",{parentName:"pre",className:"language-shell"},"source scripts/env.sh\ngit commit\ngit tag v${APP_VERSION}\ngit push --tags\n"))),Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"Monitor Travis CI at\n",Object(r.b)("a",{parentName:"p",href:"https://travis-ci.com/github/pravega/flink-tools"},"https://travis-ci.com/github/pravega/flink-tools"),".")),Object(r.b)("li",{parentName:"ol"},Object(r.b)("p",{parentName:"li"},"When complete, the installation archive will be available at\n",Object(r.b)("a",{parentName:"p",href:"https://github.com/pravega/flink-tools/releases"},"https://github.com/pravega/flink-tools/releases"),"."))),Object(r.b)("h2",{id:"references"},"References"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},Object(r.b)("a",{parentName:"li",href:"https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/connectors/streamfile_sink.html"},"Steaming File Sink"))),Object(r.b)("h2",{id:"see-also"},"See Also"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},Object(r.b)("a",{parentName:"li",href:"https://github.com/pravega/pravega-sensor-collector"},"https://github.com/pravega/pravega-sensor-collector")),Object(r.b)("li",{parentName:"ul"},Object(r.b)("a",{parentName:"li",href:"https://www.pravega.io/"},"https://www.pravega.io/"))),Object(r.b)("h2",{id:"about"},"About"),Object(r.b)("p",null,"Pravega Flink Tools is 100% open source and community-driven. All components are available\nunder ",Object(r.b)("a",{parentName:"p",href:"https://www.apache.org/licenses/LICENSE-2.0.html"},"Apache 2 License")," on GitHub."))}c.isMDXComponent=!0},160:function(e,t,a){"use strict";a.d(t,"a",(function(){return b})),a.d(t,"b",(function(){return d}));var n=a(0),l=a.n(n);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,l=function(e,t){if(null==e)return{};var a,n,l={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(l[a]=e[a]);return l}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(l[a]=e[a])}return l}var p=l.a.createContext({}),c=function(e){var t=l.a.useContext(p),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},b=function(e){var t=c(e.components);return l.a.createElement(p.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return l.a.createElement(l.a.Fragment,{},t)}},u=l.a.forwardRef((function(e,t){var a=e.components,n=e.mdxType,r=e.originalType,i=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),b=c(a),u=n,d=b["".concat(i,".").concat(u)]||b[u]||m[u]||r;return a?l.a.createElement(d,o(o({ref:t},p),{},{components:a})):l.a.createElement(d,o({ref:t},p))}));function d(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var r=a.length,i=new Array(r);i[0]=u;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o.mdxType="string"==typeof e?e:n,i[1]=o;for(var p=2;p<r;p++)i[p]=a[p];return l.a.createElement.apply(null,i)}return l.a.createElement.apply(null,a)}u.displayName="MDXCreateElement"}}]);