<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.72">
<link rel="alternate" type="application/rss+xml" href="/docs/blog/rss.xml" title="Dell EMC Streaming Data Platform Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/docs/blog/atom.xml" title="Dell EMC Streaming Data Platform Blog Atom Feed"><title data-react-helmet="true">Pravega Segment Store Service | Dell EMC Streaming Data Platform</title><meta data-react-helmet="true" property="og:url" content="https://StreamingDataPlatform.github.io//docs/docs/pravega/segment-store-service"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Pravega Segment Store Service | Dell EMC Streaming Data Platform"><meta data-react-helmet="true" name="description" content="&lt;!--"><meta data-react-helmet="true" property="og:description" content="&lt;!--"><link data-react-helmet="true" rel="shortcut icon" href="/docs/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://StreamingDataPlatform.github.io//docs/docs/pravega/segment-store-service"><link data-react-helmet="true" rel="alternate" href="https://StreamingDataPlatform.github.io//docs/docs/pravega/segment-store-service" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://StreamingDataPlatform.github.io//docs/docs/pravega/segment-store-service" hreflang="x-default"><link rel="stylesheet" href="/docs/assets/css/styles.77b8a9b9.css">
<link rel="preload" href="/docs/assets/js/styles.b1fb6875.js" as="script">
<link rel="preload" href="/docs/assets/js/runtime~main.6fd86e16.js" as="script">
<link rel="preload" href="/docs/assets/js/main.136b01c8.js" as="script">
<link rel="preload" href="/docs/assets/js/1.3bcfe3f0.js" as="script">
<link rel="preload" href="/docs/assets/js/2.9f9d1907.js" as="script">
<link rel="preload" href="/docs/assets/js/83.e85de545.js" as="script">
<link rel="preload" href="/docs/assets/js/84.8b65af75.js" as="script">
<link rel="preload" href="/docs/assets/js/935f2afb.d24e4f9c.js" as="script">
<link rel="preload" href="/docs/assets/js/17896441.654df565.js" as="script">
<link rel="preload" href="/docs/assets/js/a89a77cd.d027c928.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#main" class="skipToContent_1oUP">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle" type="button" tabindex="0"><svg aria-label="Menu" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a href="https://www.dell.com/" target="_blank" rel="noopener noreferrer" class="navbar__brand"><img src="/docs/img/logo.svg" alt="Dell Logo" class="themedImage_1VuW themedImage--light_3UqQ navbar__logo"><img src="/docs/img/logo.svg" alt="Dell Logo" class="themedImage_1VuW themedImage--dark_hz6m navbar__logo"></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/docs/sdp/overview">Docs</a><a class="navbar__item navbar__link" href="/docs/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/pravega/pravega" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub</a><div class="react-toggle react-toggle--disabled displayOnlyInLargeViewport_GrZ2"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_71bT">ðŸŒœ</span></div><div class="react-toggle-track-x"><span class="toggle_71bT">ðŸŒž</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" disabled="" aria-label="Dark mode toggle" class="react-toggle-screenreader-only"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a href="https://www.dell.com/" target="_blank" rel="noopener noreferrer" class="navbar__brand"><img src="/docs/img/logo.svg" alt="Dell Logo" class="themedImage_1VuW themedImage--light_3UqQ navbar__logo"><img src="/docs/img/logo.svg" alt="Dell Logo" class="themedImage_1VuW themedImage--dark_hz6m navbar__logo"></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/docs/docs/sdp/overview">Docs</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/blog">Blog</a></li><li class="menu__list-item"><a href="https://github.com/pravega/pravega" target="_blank" rel="noopener noreferrer" class="menu__link">GitHub</a></li></ul></div></div></div></nav><div class="main-wrapper main-docs-wrapper"><div class="docPage_31aa"><div class="docSidebarContainer_3Kbt" role="complementary"><div class="sidebar_15mo"><div class="menu menu--responsive thin-scrollbar menu_Bmed"><button aria-label="Open menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_fgN0" width="24" height="24" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Streaming Data Platform</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/sdp/overview">Overview</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/sdp/about">About</a></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!" tabindex="-1">Installation and Administration</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/sdp/install/index">Overview</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/sdp/install/guide">Guide</a></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!" tabindex="-1">Troubleshooting</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/sdp/troubleshooting/index">Overview</a></li></ul></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!" tabindex="-1">Developer&#x27;s Guide</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/sdp/developer-guide/index">Overview</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/sdp/developer-guide/guide">Guide</a></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!" tabindex="-1">Analytics</a><ul class="menu__list"><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!" tabindex="-1">Flink</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/sdp/analytics/flink/index">Overview</a></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!" tabindex="-1">Spark</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/sdp/analytics/spark/index">Overview</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/sdp/analytics/spark/deploying">Deploying</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/sdp/analytics/spark/troubleshooting">Troubleshooting</a></li></ul></li></ul></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!" tabindex="-1">Security Configuration Guide</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/sdp/security-configuration/index">Overview</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/sdp/security-configuration/guide">Guide</a></li></ul></li></ul></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Pravega</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/docs/pravega/index">Pravega Overview</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/docs/pravega/getting-started">Getting Started</a></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!" tabindex="0">Understanding Pravega</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/docs/pravega/pravega-concepts">Pravega Concepts</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/docs/pravega/terminology">Terminology</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/docs/pravega/key-features">Pravega Key Features</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/docs/pravega/faq">Frequently Asked Questions</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/docs/docs/pravega/segment-store-service">Pravega Segment Store Service</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/docs/pravega/segment-containers">Segment Containers in a Pravega Cluster</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/docs/pravega/controller-service">Pravega Controller Service</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/docs/pravega/wire-protocol">Pravega Streaming Service Wire Protocol</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/docs/pravega/state-synchronizer-design">State Synchronizer Design</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/docs/pravega/reader-group-design">Reader Groups Design</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/docs/pravega/watermarking">Watermarking</a></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!" tabindex="0">Developing Pravega Applications</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/pravega/pravega-write-read-methods">Methods for Writing and Reading Pravega Streams</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/pravega/javadoc">Java API Reference</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/pravega/rest/restapis">Pravega Controller APIs</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/pravega/connectors">Pravega Connectors</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/pravega/basic-reader-and-writer">Basic Reader and Writer</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/pravega/state-synchronizer">State Synchronizer</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/pravega/transactions">Transactions</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/pravega/streamcuts">StreamCuts</a></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!" tabindex="0">Running Pravega</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/pravega/deployment/deployment">Pravega Deployment Overview</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/pravega/deployment/manual-install">Manual Installation</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/pravega/deployment/kubernetes-install">Deploying in Kubernetes</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/pravega/deployment/docker-swarm">Deploying in a Docker Swarm</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/pravega/deployment/dcos-install">Deploying on DC/OS</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/pravega/deployment/aws-install">Running on AWS</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/pravega/metrics">Pravega Metrics</a></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!" tabindex="0">Pravega Security</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/pravega/auth/auth-plugin">Implementation of Pravega Authentication/Authorization Plugin</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/pravega/auth/client-auth">Client Auth Interface</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/pravega/security/pravega-security-authorization-authentication">TLS, Authorization, Authentication - Enabling encryption, authorization and authentication features</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/pravega/security/pravega-security-configurations">Pravega Security Configurations</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/pravega/security/pravega-security-encryption">Pravega Encryption</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/pravega/security/securing-distributed-mode-cluster">Setting Up Security for a Distributed Mode Cluster</a></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!" tabindex="0">Contributing to Pravega</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/pravega/contributing">Contributing to Pravega</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/pravega/roadmap">Pravega Roadmap</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/pravega/join-community">Join the Pravega Community</a></li></ul></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Connectors</a><ul class="menu__list"><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!" tabindex="-1">Flink</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/flink-connectors/index">Overview</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/flink-connectors/getting-started">Getting Started</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/flink-connectors/quickstart">Quick Start</a></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!" tabindex="-1">Features</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/flink-connectors/streaming">Streaming</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/flink-connectors/batch">Batch</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/flink-connectors/table-api">Table API</a></li></ul></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/flink-connectors/metrics">Metrics</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/flink-connectors/configurations">Configurations</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/flink-connectors/serialization">Serialization</a></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!" tabindex="-1">Spark</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/spark-connectors/index">Overview</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/spark-connectors/getting-started">Getting Started</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/spark-connectors/samples">Samples</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/spark-connectors/configuration">Configuration</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/spark-connectors/build-connector">Building the Connector</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs/docs/spark-connectors/limitations">Limitations</a></li></ul></li><li class="menu__list-item"><a href="https://github.com/pravega/nifi-pravega" target="_blank" rel="noopener noreferrer" class="menu__link menuLinkExternal_1OhN" tabindex="-1">NiFi</a></li><li class="menu__list-item"><a href="https://github.com/pravega?q=logstash" target="_blank" rel="noopener noreferrer" class="menu__link menuLinkExternal_1OhN" tabindex="-1">Logstash</a></li><li class="menu__list-item"><a href="https://github.com/pravega/boomi-connector" target="_blank" rel="noopener noreferrer" class="menu__link menuLinkExternal_1OhN" tabindex="-1">Boomi</a></li><li class="menu__list-item"><a href="https://github.com/pravega/kafka-adapter" target="_blank" rel="noopener noreferrer" class="menu__link menuLinkExternal_1OhN" tabindex="-1">Kafka Adapter</a></li></ul></li></ul></div><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_1CGd"><svg width="20" height="20" role="img" class="collapseSidebarButtonIcon_3E-R"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div><main class="docMainContainer_3ufF"><div class="container padding-vert--lg docItemWrapper_3FMP"><div class="row"><div class="col docItemCol_3FnS"><div class="docItemContainer_33ec"><article><header><h1 class="docTitle_3a4h">Pravega Segment Store Service</h1></header><div class="markdown"><ul><li><a href="#introduction">Introduction</a></li><li><a href="#terminology">Terminology</a></li><li><a href="#architecture">Architecture</a><ul><li><a href="#system-diagram">System diagram</a></li></ul></li><li><a href="#components">Components</a><ul><li><a href="#segment-containers">Segment Containers</a><ul><li><a href="#segment-container-metadata">Segment Container Metatdata</a><ul><li><a href="#container-metadata">Container Metadata</a></li><li><a href="#segment-metadata">Segment Metadata</a></li></ul></li></ul></li><li><a href="#log-operations">Log Operations</a></li><li><a href="#durable-log">Durable Log</a><ul><li><a href="#information-flow-in-the-durable-log">Information Flow</a></li><li><a href="#truncation">Truncation</a></li><li><a href="#operation-processor">Operation Processor</a></li><li><a href="#operation-metadata-updater">Operation Metadata Updater</a></li><li><a href="#durable-data-log">Durable Data Log</a></li><li><a href="#in-memory-operation-log">In-Memory Operation Log</a></li></ul></li><li><a href="#read-index">Read Index</a></li><li><a href="#cache">Cache</a></li><li><a href="#storage-writer">Storage Writer</a></li></ul></li><li><a href="#integration-with-controller">Integration with Controller</a><ul><li><a href="#segment-container-manager">Segment Container Manager</a></li></ul></li><li><a href="#storage-abstractions">Storage Abstraction</a></li><li><a href="#data-flow">Data Flow</a><ul><li><a href="#appends">Appends</a></li><li><a href="#reads">Reads</a></li><li><a href="#synchronization-with-tier-2-storage-writer">Synchronization with Tier 2</a></li><li><a href="#container-startup-normalrecovery">Container Startup</a></li></ul></li></ul><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="introduction"></a>Introduction<a class="hash-link" href="#introduction" title="Direct link to heading">#</a></h1><p>The Pravega Segment Store Service is a subsystem that lies at the heart of the entire Pravega deployment. It is the main access point for managing <strong>Stream Segments</strong>, providing the ability to <em>create</em>, <em>delete</em> and <em>modify/access</em> their contents. The Pravega client communicates with the Pravega Stream Controller to figure out which Stream Segments need to be used (for a Stream), and both the Stream Controller and the client deal with the Segment Store Service to operate on them.</p><p>The basic idea behind the Segment Store Service is that it buffers the incoming data in a very fast and durable append-only medium (Tier 1), and syncs it to a high-throughput (but not necessarily low latency) system (Tier 2) in the background, while aggregating multiple (smaller) operations to a Stream Segment into a fewer (but larger) ones.</p><p>The Pravega Segment Store Service can provide the following guarantees:</p><ul><li>Stream Segments that are unlimited in length, with append-only semantics, yet supporting arbitrary-offset reads.</li><li>No throughput degradation when performing small appends, regardless of the performance of the underlying Tier 2 Storage system.</li><li>Multiple concurrent writers to the same Stream Segment.</li><li>the order is guaranteed within the context of a single Writer, but appends from multiple concurrent Writers will be added in the order in which they were received (appends are atomic without interleaving their contents).</li><li>Writing to and reading from a Stream Segment concurrently with relatively low latency between writing and reading.</li></ul><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="terminology"></a>Terminology<a class="hash-link" href="#terminology" title="Direct link to heading">#</a></h1><p>The following terminologies are used throughout the document:</p><ul><li><strong>Stream Segment</strong> or <strong>Segment</strong>: A contiguous sequence of bytes, similar to a file of unbounded size. This is a part of a Stream, limited both temporally and laterally (by key). The scope of Streams and mapping Stream Segments to such Streams is beyond the purpose of this document.</li><li><strong>Tier 2 Storage</strong> or <strong>Permanent Storage</strong>: The final resting place of the data.</li><li><strong>Tier 1 Storage</strong>: Fast append storage, used for durable buffering of incoming appends before distributing to Tier 2 Storage.</li><li><strong>Cache</strong>: A key-value local cache with no expectation of durability.</li><li><strong>Pravega Segment Store Service</strong> or <strong>Segment Store</strong>: The Service that this document describes.</li><li><strong>Transaction</strong>: A sequence of appends that are related to a Segment, which, if persisted, would make up a contiguous range of bytes within it. This is used for ingesting very large records or for accumulating data that may or may not be persisted into the Segment (but its fate cannot be determined until later in the future).</li></ul><p><strong>Note:</strong> At the Pravega level, a Transaction applies to an entire Stream. In this document, a Transaction applies to a single Segment.</p><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="architecture"></a>Architecture<a class="hash-link" href="#architecture" title="Direct link to heading">#</a></h1><p>The <strong>Segment Store</strong> is made up of the following components:</p><ul><li><strong>Pravega Node</strong>: A host running a Pravega Process.</li><li><strong>Stream Segment Container</strong> (or <strong>Segment Container</strong>): A logical grouping of Stream Segments. The mapping of Segments to Containers is deterministic and does not require any persistent store; Segments are mapped to Containers via a hash function (based on the Segment&#x27;s name).</li><li><strong>Durable Data Log Adapter</strong> (or <strong>Durable Data Log</strong>): An abstraction layer for Tier 1 Storage.</li><li><strong>Storage Adapter</strong>: An abstraction layer for Tier 2 Storage.</li><li><strong>Cache</strong>: An abstraction layer for append data caching.</li><li><strong>Streaming Client</strong>: An API that can be used to communicate with the Pravega Segment Store.</li><li><strong>Segment Container Manager</strong>: A component that can be used to determine the lifecycle of Segment Containers on a Pravega Node. This is used to start or stop Segment Containers based on an external coordination service (such as the Pravega Controller).</li></ul><p>The Segment Store handles writes by first writing them to a log (<em>Durable Data Log</em>) on a fast storage (SSDs preferably) and immediately acking back to the client after they have been persisted there. Subsequently, those writes are then aggregated into larger chunksÂ and written in the background to Tier 2 Storage. Data for appends that have been acknowledged (and are in Tier 1) but not yet in Tier 2 is stored in the Cache (in addition to Tier 1). Once such data has been written to Tier 2 Storage, it may or may not be kept in the Cache, depending on a number of factors, such as Cache utilization/pressure and access patterns.</p><p>More details about each component described above can be found in the <a href="#components">Components</a> section.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="system-diagram"></a>System Diagram<a class="hash-link" href="#system-diagram" title="Direct link to heading">#</a></h2><p><img alt="System Diagram" src="/docs/assets/images/Segment-store-components-711591d6b64b575fa80c8a4dc3f33417.png"></p><p>In the above diagram, the major components of the Segment Store are shown. But for simplicity, only one Segment Container is depicted. All Container components and major links between them (how they interact with each other) are shown. The <em>Container Metadata</em> component is not shown, because every other component communicates with it in one form or another and adding it would only clutter the diagram.</p><p>More detailed diagrams can be found under the <a href="#data-flow">Data Flow</a> section.</p><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="components"></a>Components<a class="hash-link" href="#components" title="Direct link to heading">#</a></h1><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="segment-containers"></a>Segment Containers<a class="hash-link" href="#segment-containers" title="Direct link to heading">#</a></h2><p>Segment Containers are a logical grouping of Segments and are responsible for all operations on those Segments within their span. A Segment Container is made of multiple sub-components:</p><ul><li><strong>Segment Container Metadata</strong>: A collection of Segment-specific metadata that describes the current state of each Segment (how much data in Tier 2, how much in Tier 1, whether it is sealed, etc.), as well as other miscellaneous info about each Container.</li><li><strong>Durable Log</strong>: The Container writes every operation it receives to this log and acks back only when the log says it has been accepted and durably persisted.</li><li><strong>Read Index</strong>:Â An in-memory index of where data can be read from. The Container delegates all read requests to it, and it is responsible for fetching the data from wherever it is currently located (Cache, Tier 1 Storage or Tier 2 Storage).</li><li><strong>Cache</strong>:Â Used to store data for appends that exist in Tier 1 only (not yet in Tier 2), as well as blocks of data that support reads.</li><li><strong>Storage Writer</strong>:Â Processes the durable log operations and applies them to Tier 2 Storage (in the order in which they were received). This component is also the one that coalesces multiple operations together, for better back-end throughput.</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="segment-container-metadata"></a>Segment Container Metadata<a class="hash-link" href="#segment-container-metadata" title="Direct link to heading">#</a></h3><p>The Segment Container Metadata is critical to the good functioning and synchronization of its components. This metadata is shared across all components and it comes at two levels: &quot;Container-wide metadata&quot; and &quot;per-Segment metadata&quot;. Each serves a different purpose and is described below.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="container-metadata"></a>Container Metadata<a class="hash-link" href="#container-metadata" title="Direct link to heading">#</a></h4><p>Each <strong>Segment Container</strong> needs to keep some general-purpose metadata that affects all operations inside the container:</p><ul><li><strong>Operation Sequence Number</strong>:Â The largest sequence number assigned by the <em>Durable Log</em>. Every time a new operation is received and successfully processed by the <em>Durable Log</em>, this number is incremented (its value will never decrease or otherwise rollback, even if an operation failed to be persisted).<ul><li>The operation sequence number is guaranteed to be strict-monotonic increasing (no two operations have the same value, and an operation will always have a larger sequence number than all operations before it).</li></ul></li><li><strong>Epoch</strong>: A number that is incremented every time a successful recovery (Container Start) happens. This value is durably incremented and stored as part of recovery and can be used for a number of cases (a good use is Tier 2 fencing for HDFS, which doesn&#x27;t provide a good, native mechanism for that).</li><li><strong>Active Segment Metadata</strong>: Keeps information about each active Stream Segment. A Segment is active if it has had activity (read or write) recently and is currently loaded in memory. If a Stream Segment is idle for a while, or if there are many Stream Segments currently active, a Stream Segment becomes inactive by having its outstanding metadata flushed to Tier 2 Storage and evicted from memory.</li><li><strong>Tier 1 Metadata</strong>: Various pieces of information that can be used to accurately truncate the Tier 1 Storage Log once all operations prior to that point have been durably stored to Tier 2.</li><li><strong>Checkpoints</strong>: Container metadata is periodically Checkpointed by having its entire snapshot (including Active Segments) serialized to Tier 1. A Checkpoint serves as a Truncation Point for Tier 1, as it contains all the updates that have been made to the Container via all the processed operations before it, so we no longer need those operations in order to reconstruct the metadata. If we truncate Tier 1 on a Checkpoint, then we can use information from Tier 2 and this Checkpoint to reconstruct by using the previously available metadata, without relying on any operation prior to it in Tier 1.</li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="segment-metadata"></a>Segment Metadata<a class="hash-link" href="#segment-metadata" title="Direct link to heading">#</a></h4><p>Each Segment Container needs to keep per-segment metadata, which it uses to keep track of the state of each segment as it processes operations for it. The metadata can be volatile (it can be fully rebuilt upon recovery) and contains the following properties for each segment that is currently in use:</p><ul><li><code>Name</code>: The name of the Stream Segment.</li><li><code>Id</code>: Internally assigned unique Stream Segment ID. This is used to refer to Stream Segments, which is preferred to the Name. This ID is used for the entire lifetime of the Stream Segment, which means that even if the Stream Segment becomes inactive, a future reactivation will have it mapped to the same ID.</li><li><code>StartOffset</code> (also known as <code>TruncationOffset</code>): The lowest offset of the data that is available for reading. A non-truncated Stream Segment will have Start Offset equal to <em>0</em>, while subsequent Truncate operations will increase (but never decreases) this number.</li><li><code>StorageLength</code>: The highest offset of the data that exists in Tier 2 Storage.</li><li><code>Length</code>: The highest offset of the committed data inÂ Tier 1 Storage.</li><li><code>LastModified</code>: The timestamp of the last processed (and acknowledged) append.</li><li><code>IsSealed</code>: Whether the Stream Segment is closed for appends (this value may not have been applied to Tier 2 Storage yet).</li><li><code>IsSealedInStorage</code>: Whether the Stream Segment is closed for appends (and this has been persisted in Tier 2 Storage).</li><li><code>IsMerged</code>: Whether the Stream Segment has been merged into another one (but this has not yet been persisted in Tier 2 Storage). This only applies for Transactions. Once the merge is persisted into Tier 2, the Transaction Segment does not exist anymore (so <code>IsDeleted</code> will become true).</li><li><code>IsDeleted</code>: Whether the Stream Segment is deleted or has recently been merged into another Stream Segment. This only applies for recently deleted Stream Segments, and not for Stream Segments that never existed.</li></ul><p>The following areÂ alwaysÂ <strong>true</strong> for any Stream Segment:</p><ul><li><code>StorageLength</code>Â &lt;=Â <code>Length</code></li><li><code>StartOffset</code>Â &lt;=Â <code>Length</code></li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="log-operations"></a>Log Operations<a class="hash-link" href="#log-operations" title="Direct link to heading">#</a></h3><p>The <strong>Log Operation</strong> is a basic unit that is enqueued in the <em>Durable Log</em>. It does not represent an action, per se, but is the base for several serializable operations (we can serialize multiple types of operations, not just Appends). Each operation is the result of an external action (which denote the alteration of a Stream Segment), or an internal trigger, such as metadata maintenance operations.</p><p>Every Log operation has the following elements:</p><ul><li><code>SequenceNumber</code>: The unique sequence number assigned to this entry (see more under <a href="#segment-container-metadata">Container Metadata</a>) section.</li></ul><p>The following are the various types of Log operations:</p><ul><li><strong>Storage Operations</strong>: Represent operations that need to be applied to the underlying Tier 2 Storage:<ul><li><code>StreamSegmentAppendOperation</code>: Represents an append to a particular Stream Segment.</li><li><code>CachedStreamSegmentAppendOperation</code>: Same as <code>StreamSegmentAppendOperation</code>, but this is for internal use (instead of having an actual data payload, it points to a location in the Cache from where the data can be retrieved).</li><li><code>StreamSegmentSealOperation</code>:Â When processed, it sets a flag in the in-memory metadata that no more appends can be received. When theÂ Storage WriterÂ processes it, it marks the Stream Segment as read-only in Tier 2 Storage.</li><li><code>StreamSegmentTruncateOperation</code>: Truncates a Stream Segment at a particular offset. This causes the Stream Segment&#x27;s <code>StartOffset</code> to change.</li><li><code>MergeTransactionOperation</code>: Indicates that a Transaction is to be merged into its parent Stream Segment.</li></ul></li><li><strong>Metadata Operations</strong> are auxiliary operations that indicate a change to the Container metadata. They can be the result of an external operation (we received a request for a Stream Segment we never knew about before, so we must assign a &quot;unique ID&quot; to it) or to snapshot the entire metadata (which helps with recovery and cleaning up Tier 1 Storage). The purpose of the metadata operations is to reduce the amount of time needed for failover recovery (when needed).<ul><li><code>StreamSegmentMapOperation</code>:Â Maps an ID to a Stream Segment Name.</li><li><code>TransactionMapOperation</code>:Â Maps an ID to a Transaction and to its Parent Segment.</li><li><code>UpdateAttributesOperation</code>: Updates any attributes on a Stream Segment.</li><li><code>MetadataCheckpoint</code>: Includes an entire snapshot of the metadata. This can be useful during recovery. This contains all metadata up to this point, which is a sufficient base for all operations after it.</li></ul></li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="durable-log"></a>Durable Log<a class="hash-link" href="#durable-log" title="Direct link to heading">#</a></h3><p>The <em>Durable Log</em> is the central component that handles all Log operations. All operations (which are created by the Container) are added to the <em>Durable Log</em>, which processes them in the order in which they were received. It is made up of a few other components, all of which are working towards a single goal of processing all incoming operations as quickly as possible, without compromising data integrity.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="information-flow-in-the-durable-log"></a>Information Flow in the Durable Log<a class="hash-link" href="#information-flow-in-the-durable-log" title="Direct link to heading">#</a></h4><ol><li><p>All received operations are added to an <em>Operation Queue</em> (the caller receives a Future which will be completed when the operation is durably persisted).</p></li><li><p>The <em>Operation Processor</em> picks all operations currently available in the queue (if the queue is empty, it will wait until at least one operation is added).</p></li><li><p>The <em>Operation Processor</em> runs as a continuous loop (in a background thread), and executes the following steps.</p><ol><li><p><em>Dequeue</em> all outstanding operations from the operation Queue (described above).</p></li><li><p><em>Pre-process</em> the operations (validate that they are correct and would not cause undesired behavior, assign offsets (where needed), assign sequence numbers, etc.)</p></li><li><p><em>Write</em> the operations to a <em>Data Frame Builder</em>, which serializes and packs the operations in <em>Data Frames</em>. Once a <em>Data Frame</em> is complete (full or no more operations to add), the <em>Data Frame</em> Builder sends the <em>Data Frame</em> to the <em>Durable Data Log</em>. Note that, an operation may span multiple <em>DataFrames</em>, but the goal is to make the best use of the <em>Durable Data Log</em> throughput capacity by making writes as large as possible considering the maximum size limit per write.</p></li></ol></li><li><p>When a <em>Data Frame</em> has been durably persisted in the <em>Durable Data Log</em>, the operation Processor post-processes all operations that were fully written so far. It adds them to in-memory structures, updates indices, etc., and completes the Futures associated with them.</p></li><li><p>The <em>Operation Processor</em> works asynchronously, by not waiting for a particular <em>Data Frame</em> to be written before starting another one and sending it to the <em>Durable Data Log</em>. Likewise, multiple <em>Data Frames</em> may be in flight by maintaining a specific order. The operation Processor relies on certain ordering guarantees from the <em>Durable Data Log</em>, if a particular <em>Data Frame</em> was acked, it assures that all the prior <em>Data Frames</em> to it were also committed successfully, in the right order.  </p></li></ol><p><strong>Note:</strong> The operation Processor does not do any write throttling. It leaves that to the <em>Durable Data Log</em> implementation, but it controls the size of the Data Frames that get sent to it.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="truncation"></a>Truncation<a class="hash-link" href="#truncation" title="Direct link to heading">#</a></h4><p>Based on supplied configuration, the <em>Durable Log</em> auto-adds a special kind of operation, named <code>MetadataCheckpointOperation</code>. This operation, when processed by the operation Processor, collects a snapshot of the entire Container metadata and serializes it to the <em>Durable Data Log</em>. This special operation marks a <strong>Truncation Point</strong> - a place in the Stream of Log operations where we can issue Truncate operations. It is very important that after every truncation, the first operation to be found in the log is a <code>MetadataCheckpointOperation</code>, because without the prior operations to reconstruct metadata, this is the only way to be able to process subsequent operations.</p><p><strong>Note:</strong> <em>Durable Data Log</em> (Tier 1) truncation should not be confused with Segment Truncation. They serve different purposes and are applied to different targets.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="operation-processor"></a>Operation Processor<a class="hash-link" href="#operation-processor" title="Direct link to heading">#</a></h4><p>The <em>Operation Processor</em> is a sub-component of the <em>Durable Log</em> that deals with incoming Log operations. Its purpose is to <em>validate, persist</em>, and <em>update</em> metadata and other internal structures based on the contents of each operation.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="operation-metadata-updater"></a>Operation Metadata Updater<a class="hash-link" href="#operation-metadata-updater" title="Direct link to heading">#</a></h4><p>The <em>Operation Metadata Updater</em> is a sub-component of the <em>Durable Log</em> that is responsible with validating operations based on the current state of the metadata, as well as update the metadata after a successful commit of an operation. Internally it has various mechanisms to handle failures, and it can rollback certain changes in failure situations.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="durable-data-log"></a>Durable Data Log<a class="hash-link" href="#durable-data-log" title="Direct link to heading">#</a></h4><p>The <em>Durable Data Log</em> is an abstraction layer to an external component that provides append-only semantics. It is supposed to be a system which provides very fast appends to a log, that guarantees the durability and consistency of the written data. The read performance is not so much a factor, because we do not read directly from this component. Read is performed on it when we need to recover the contents of the <em>Durable Log</em>.</p><p>As explained above, Log operations are serialized into <em>Data Frames</em> (with a single operation able to span multiple such Frames if needed), and these <em>Data Frames</em> are then serialized as entries into this <em>Durable Data Log</em>. This is used only as a fail-safe, and we only need to read these Frames back if we need to perform recovery (in which case we need to deserialize all Log operations contained in them, in the same order in which they were received).</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="in-memory-operation-log"></a>In-Memory Operation Log<a class="hash-link" href="#in-memory-operation-log" title="Direct link to heading">#</a></h4><p>The <em>In-Memory Operation Log</em> contains committed (and replicated) Log operationsÂ in the exact same order as they were added to the <em>Durable Data Log</em>. While the <em>Durable Data Log</em> contains a sequence of Data Frames (which contain serializations of operations), the Memory Log contains the actual operations, which can be used throughout the <em>Durable Log</em> and the Storage Writer.</p><p>The Memory Log is essentially a chain of Log operationsÂ ordered by the time when the operationÂ was received. We always add at one end, and we removeÂ from the other. When we truncate the <em>Durable Data Log</em> the Memory Log is also truncated at the same location.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="read-index"></a>Read Index<a class="hash-link" href="#read-index" title="Direct link to heading">#</a></h3><p>The <em>Read Index</em> helps the Segment Container perform reads from Streams at arbitrary offsets. While the <em>Durable Log</em> records (and can only replay) data in the order in which it is received, the <em>Read Index</em> can access the data in a random fashion. The <em>Read Index</em> is made of multiple <em>Segment Read Indices</em> (one per live Segment).</p><p>TheÂ <em>Segment Read Index</em>Â is a data structure that is used to serve reads from memory, as well as pull data from Tier 2 Storage and provides <em>Future Reads</em> (tail reads) when data is not yet available. When a read request is received, the <em>Segment Read Index</em> returns a read iterator that will return data as long as the read request parameters have not yet been satisfied. The iterator will either fetch data that is immediately available in memory, or request data from Tier 2 storage (and bring it to memory) or, if it reached the current end of the Segment, return a Future that will be completed when new data is added (thus providing tailing or future reads).</p><p>At the heart of the <em>Segment Read Index</em> lies a sorted index of entries (indexed by their start offsets) which is used to locate the requested data when needed. The index itself is implemented by a custom balanced binary search tree (AVL Tree to be more precise) with a goal of minimizing memory usage while not sacrificing insert or access performance. The entries themselves do not contain data, rather some small amount of metadata that is used to locate the data in the Cache and to determine usage patterns (good for cache evictions).</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="cache"></a>Cache<a class="hash-link" href="#cache" title="Direct link to heading">#</a></h3><p>The Cache is a component where all data (whether from new appends or that was pulled from Tier 2 storage) is stored. It is a direct memory store entirely managed by the Read Index.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="storage-writer"></a>Storage Writer<a class="hash-link" href="#storage-writer" title="Direct link to heading">#</a></h3><p>Pravega is by no means the final resting place of the data, nor it is meant to be a storage service. The Tier 2 Storage is where we want data to be in the long term and Pravega is only used to store a very short tail-end of it (using Tier 1 Storage), enough to make appends fast and aggregate them into bigger chunks for committal to Tier 2 Storage. To perform this, it needs another component (<strong>Storage Writer</strong>) that reads data from the <em>Durable Log</em> in the order in which it was received, aggregates it, and sends it to Tier 2 Storage.</p><p>Just like the <em>Durable Log</em>, there is one Storage WriterÂ per Segment Container. Each WriterÂ reads LogÂ operations from the in-memory operation Log (exposed via the <code>read()</code> method in the <em>Durable Log</em>) in the order they were processed. It keeps track of the last read item by means of its sequence number. This state is not persisted, and upon recovery, it can just start from the beginning of the available <em>Durable Log</em>.</p><p>The Storage Writer can process any Storage operation (<em>Append, Seal, Merge</em>), and as Pravega being the sole actor it modifies such data in Tier 2 and applies them without any constraints. It has several mechanisms to detect and recover from possible data loss or external actors modifying data concurrently.</p><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="integration-with-controller"></a>Integration with Controller<a class="hash-link" href="#integration-with-controller" title="Direct link to heading">#</a></h1><p>Methods for mapping Segment Containers to hosts and rules used for moving from one to another are beyond the scope of this document. Here, we just describe how the Segment Store Service interacts with the <em>Controller</em> and how it manages the lifecycle of Segment Containers based on external events.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="segment-container-manager"></a>Segment Container Manager<a class="hash-link" href="#segment-container-manager" title="Direct link to heading">#</a></h2><p>Each instance of a Segment Store Service needs a <em>Segment Container Manager</em>. The role of this component is to manage the lifecycle of the Segment Containers that are assigned to that node (service). It performs the following duties:</p><ul><li>Connects to the Controller Service-Side Client (i.e., a client that deals only with Segment Container events, and not with the management of Streams and listens to all changes that pertain to Containers that pertain to its own instance.</li><li>When it receives a notification that it needs to start a Segment Container for a particular Container Id, it initiates the process of bootstrapping such an object. It does not notify the requesting client of success until the operation completes without error.</li><li>When it receives a notification that it needs to stop a Segment Container for a particular Container Id, it initiates the process of shutting it down. It does not notify the requesting client of success until the operation completes without error.</li><li>If the Segment Container shuts down unexpectedly (whether during Start or during its normal operation), it will not attempt to restart it locally; instead it will notify the Controller.</li></ul><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="storage-abstractions"></a>Storage Abstractions<a class="hash-link" href="#storage-abstractions" title="Direct link to heading">#</a></h1><p>The Segment Store was not designed with particular implementations for Tier 1 or Tier 2. Instead, all these components have been abstracted out in simple, well-defined interfaces, which can be implemented against any standard file system (Tier 2) or append-only log system (Tier 1).</p><p>Possible candidates for Tier 1 Storage:</p><ul><li><strong>Apache BookKeeper</strong> (preferred, adapter is fully implemented as part of Pravega)</li><li>Non-durable, non-replicated solutions:<ul><li>In-Memory (Single node deployment only - Pravega becomes a volatile buffer for Tier 2 Storage; data loss is unavoidable and unrecoverable from in the case of process crash or system restart).<ul><li>This is used for unit test only.</li></ul></li><li>Local File System (Single node deployment only - Pravega becomes a semi-durable buffer for Tier 2 Storage; data loss is unavoidable and unrecoverable from in the case of complete node failure)</li></ul></li></ul><p>Possible candidates for Tier 2 Storage:</p><ul><li><strong>HDFS</strong> (Implementation available)</li><li><strong>Extended S3</strong> (Implementation available)</li><li><strong>NFS</strong> (general <strong>FileSystem</strong>) (Implementation available)</li><li>In-Memory (Single node deployment only - limited usefulness; data loss is unavoidable and unrecoverable from in the case of process crash or system restart)<ul><li>This is used for unit test only.</li></ul></li></ul><p>A note about <strong>Tier 2 Truncation</strong>:</p><ul><li>The Segment Store supports Segment truncation at a particular offset, which means that, once that request is complete, no offset below that one will be available for reading.</li><li>The above is a metadata update operation, however this also needs to be supported by Tier 2 so that the truncated data is physically deleted from it.</li><li>If a Tier 2 implementation does not natively support truncation from the beginning of a file with offset preservation (i.e., a Segment of length 100 is truncated at offset 50, then offsets 0..49 are deleted, but offsets 50-99 are available and are not shifted down), then the <strong>Segment Store</strong> provides a wrapper on top of a generic Tier 2 implementation that can do that.</li><li>The <code>RollingStorage</code> Tier 2 wrapper splits a Segment into multiple <em>Segment Chunks</em> and exposes them as a single Segment to the upper layers. <em>Segment Chunks</em> that have been truncated out, are deleted automatically. This is not a very precise application (since it relies heavily on a rollover policy dictating granularity), but it is a practical solution for those cases when the real Tier 2 implementation does not provide the features that we need.  </li></ul><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="data-flow"></a>Data Flow<a class="hash-link" href="#data-flow" title="Direct link to heading">#</a></h1><p>Here are a few examples of how data flows inside the Pravega Segment Store Service.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="appends"></a>Appends<a class="hash-link" href="#appends" title="Direct link to heading">#</a></h2><p><img alt="Segment Store Appends" src="/docs/assets/images/segment-store-append-00ad0f792356573419bbce79d897d1e3.png"></p><p>The diagram above depicts these steps (note the step numbers may not match, but the order is the same):</p><ol><li><strong>Segment Store</strong> receives append request with params: Segment Name, Payload and Attribute Updates.</li><li><strong>Segment Store</strong> determines the Container ID for the given Segment and verifies that the <strong>Segment Container</strong> is registered locally. If not, it returns an appropriate error code.</li><li><strong>Segment Store</strong> delegates request to the appropriate <strong>Segment Container</strong> instance.<ol><li><strong>Segment Container</strong> verifies that the Segment belongs to the Segment Container and that the Segment actually exists. If not, it returns an appropriate error code.<ul><li>During this process, it also gets an existing Segment ID or assigns a new one (by using the <strong>Segment Mapper</strong> component).</li></ul></li><li>Segment Container creates a <code>StreamSegmentAppendOperation</code> with the input data and sends it to the <em>Durable Log</em>.</li></ol></li><li><strong>Durable Log</strong> takes the Append operation and processes it according to the algorithm described in theÂ <a href="#durable-log">Durable Log</a> section.  <ol><li>Puts it in its operation Queue.</li><li>Operation Processor pulls all operations off the Queue.</li><li>Operation Processor uses the <em>Data Frame Builder</em> to construct <em>Data Frames</em> with the operations it has.</li><li><em>Data Frame Builder</em> asynchronously writes the <em>Data Frame</em> to the <em>Durable Data Log</em>.</li><li>Upon completion, the following are done in parallel:<ul><li>Metadata is updated.</li><li>The operation is added to the <em>Memory Operation Log</em> and <em>Read Index</em>.</li><li>A call that triggered the operation is acked.</li></ul></li><li>The above process is asynchronous, which means the Operation Processor will have multiple <em>Data Frames</em> in flight (not illustrated). It will keep track of each one&#x27;s changes and apply or roll them back as needed.</li></ol></li></ol><p>This process applies for every single operation that the <strong>Segment Store</strong> supports. All <em>modify</em> operations go through the Operation Processor and have a similar path.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="reads"></a>Reads<a class="hash-link" href="#reads" title="Direct link to heading">#</a></h2><p><img alt="Segment Store Reads" src="/docs/assets/images/segment-store-reads-7392946bc98ad7edcbe07c8ff3d548c5.png"></p><p>The diagram above depicts these steps (note the step numbers may not match, but the order is the same):</p><ol><li><strong>Segment Store</strong> receives read request with params: Segment Name, Read Offset, Max-Length.<ol><li><strong>Segment Store</strong> determines the Container ID for the given Segment and verifies if it is Leader for given <strong>Segment Container</strong>. If not, it returns an appropriate error code.</li><li><strong>Segment Store</strong> delegates request to the <strong>Segment Container</strong> instance.</li></ol></li><li><strong>Segment Container</strong> verifies that the Segment belongs to that Container and that it actually exists. If not, it returns an appropriate error code to the client.<ul><li>During this process, it also gets an existing Segment ID or assigns a new one (by using the <strong>Segment Mapper</strong> component).</li></ul></li><li><strong>Segment Container</strong> delegates the request to its <em>Read Index</em>, which processes the read as described in the <a href="#read-index">Read Index</a> section, by issuing Reads from <strong>Storage</strong> (for data that is not in the <strong>Cache</strong>), and querying/updating the <strong>Cache</strong> as needed.</li></ol><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="synchronization-with-tier-2-storage-writer"></a>Synchronization with Tier 2 (Storage Writer)<a class="hash-link" href="#synchronization-with-tier-2-storage-writer" title="Direct link to heading">#</a></h2><p><img alt="Segment Store Sync Tier 2" src="/docs/assets/images/segment-store-synctier2-b7676e70c2e313dce7c78be5a4f1b3e4.png"></p><p>The diagram above depicts these steps (note the step numbers may not match, but the order is the same):</p><ol><li>The <strong>Storage Writer</strong>&#x27;s main loop is the sub-component that triggers all these operations.</li><li>Read next operation from the <em>Durable Log</em> (in between each loop, the Writer remembers what the sequence number of the last processed operation was).</li><li>All operations are processed and added to the internal <strong>Segment Aggregators</strong> (one Aggregator per Segment).</li><li>Eligible Segment Aggregators are flushed to <strong>Storage</strong> (eligibility depends on the amount of data collected in each aggregator, and whether there are any Seal, Merge or Truncate operations queued up).<ul><li>Each time an Append operation is encountered, a trip to the <em>Read Index</em> may be required in order to get the contents of the append.</li></ul></li><li>After every successful modification (<em>write/seal/concat/truncate</em>) to <strong>Storage</strong>, the <strong>Container Metadata</strong> is updated to reflect the changes.</li><li>The <em>Durable Log</em> is truncated (if eligible).</li></ol><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="container-startup-normalrecovery"></a>Container Startup (Normal/Recovery)<a class="hash-link" href="#container-startup-normalrecovery" title="Direct link to heading">#</a></h2><p><img alt="Segment Store Container Startup" src="/docs/assets/images/segment-store-recovery-47bbea320bc338b23802f694cfc7278d.png"></p><p>The diagram above depicts these steps (note the step numbers may not match, but the order is the same):</p><ol><li>The <strong>Container Manager</strong> receives a request to start a Container in this instance of the <strong>Segment Store Service</strong>.<ul><li>It creates, registers, and starts the Container.</li></ul></li><li>The <strong>Container</strong> starts the <em>Durable Log</em> component.</li><li><em>Durable Log</em> initiates the recovery process (coordinated by the <strong>Recovery Executor</strong>).</li><li><strong>Recovery Executor</strong> reads all <em>Data Frames</em> from <em>Durable Data Log</em>.</li><li>Deserialized operations from the read <em>Data Frames</em> are added to the <strong>Memory Operation Log</strong>.</li><li>The <strong>Container Metadata</strong> is updated by means of the <strong>Operation Metadata Updater</strong> (same as the one used inside Operation Processor).</li><li>The <em>Read Index</em> is populated with the contents of those operations that apply to it.</li><li>The <strong>Container</strong> Starts the <strong>Storage Writer</strong>.<ul><li>The <strong>Storage Writer</strong>&#x27;s Main Loop starts processing operations from the <em>Durable Log</em>, and upon first encountering a new Segment, it reconciles its content (and metadata) with the reality that exists in <strong>Storage</strong>.</li></ul></li><li>After both the <strong>Durable Log</strong> and the <strong>Storage Writer</strong> have started, the <strong>Container</strong> is ready to start accepting new external requests.</li></ol></div></article><div class="margin-vert--xl"><div class="row"><div class="col"><a href="https://github.com/claudiofahey/pravega/edit/docusaurus/documentation/src/docs/segment-store-service.md" target="_blank" rel="noreferrer noopener"><svg fill="currentColor" height="1.2em" width="1.2em" preserveAspectRatio="xMidYMid meet" role="img" viewBox="0 0 40 40" class="iconEdit_2_ui" aria-label="Edit page"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/docs/pravega/faq"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« Frequently Asked Questions</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/docs/pravega/segment-containers"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Segment Containers in a Pravega Cluster Â»</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_35-E thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#system-diagram" class="table-of-contents__link">System Diagram</a></li><li><a href="#segment-containers" class="table-of-contents__link">Segment Containers</a><ul><li><a href="#segment-container-metadata" class="table-of-contents__link">Segment Container Metadata</a></li><li><a href="#log-operations" class="table-of-contents__link">Log Operations</a></li><li><a href="#durable-log" class="table-of-contents__link">Durable Log</a></li><li><a href="#read-index" class="table-of-contents__link">Read Index</a></li><li><a href="#cache" class="table-of-contents__link">Cache</a></li><li><a href="#storage-writer" class="table-of-contents__link">Storage Writer</a></li></ul></li><li><a href="#segment-container-manager" class="table-of-contents__link">Segment Container Manager</a></li><li><a href="#appends" class="table-of-contents__link">Appends</a></li><li><a href="#reads" class="table-of-contents__link">Reads</a></li><li><a href="#synchronization-with-tier-2-storage-writer" class="table-of-contents__link">Synchronization with Tier 2 (Storage Writer)</a></li><li><a href="#container-startup-normalrecovery" class="table-of-contents__link">Container Startup (Normal/Recovery)</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><h4 class="footer__title">Docs</h4><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/sdp/overview">SDP Docs</a></li><li class="footer__item"><a href="https://www.dell.com/support/kbdoc/en-us/000124079/dell-emc-streaming-data-platform-infohub" target="_blank" rel="noopener noreferrer" class="footer__link-item">SDP InfoHub</a></li></ul></div><div class="col footer__col"><h4 class="footer__title">Community</h4><ul class="footer__items"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/pravega" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow</a></li><li class="footer__item"><a href="https://discordapp.com/invite/pravega" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord</a></li><li class="footer__item"><a href="https://twitter.com/pravega" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter</a></li></ul></div><div class="col footer__col"><h4 class="footer__title">More</h4><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/blog">SDP Blog</a></li><li class="footer__item"><a href="https://blog.pravega.io/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Pravega Blog</a></li><li class="footer__item"><a href="https://github.com/pravega/pravega" target="_blank" rel="noopener noreferrer" class="footer__link-item">Pravega GitHub</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2021 Dell</div></div></div></footer></div>
<script src="/docs/assets/js/styles.b1fb6875.js"></script>
<script src="/docs/assets/js/runtime~main.6fd86e16.js"></script>
<script src="/docs/assets/js/main.136b01c8.js"></script>
<script src="/docs/assets/js/1.3bcfe3f0.js"></script>
<script src="/docs/assets/js/2.9f9d1907.js"></script>
<script src="/docs/assets/js/83.e85de545.js"></script>
<script src="/docs/assets/js/84.8b65af75.js"></script>
<script src="/docs/assets/js/935f2afb.d24e4f9c.js"></script>
<script src="/docs/assets/js/17896441.654df565.js"></script>
<script src="/docs/assets/js/a89a77cd.d027c928.js"></script>
</body>
</html>